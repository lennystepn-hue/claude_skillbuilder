{
  "id": "ai-model-evaluation",
  "name": "model-evaluation",
  "description": "Evaluate ML models with proper metrics and validation",
  "category": "AI",
  "content": "---\nname: model-evaluation\ndescription: Evaluate ML models properly\ncategory: AI\n---\n\n# Model Evaluation\n\n## Overview\nComprehensively evaluate machine learning models using appropriate metrics, validation strategies, and diagnostic tools.\n\n## Activation\nActivates when user mentions \"model evaluation\", \"metrics\", \"accuracy\", \"precision\", \"recall\", \"F1\", \"AUC\", \"cross-validation\", or \"model performance\".\n\n## Instructions\n\n### Choosing the Right Metrics\n\n**Classification:**\n- Accuracy: Only for balanced datasets\n- Precision: When false positives are costly (spam detection)\n- Recall: When false negatives are costly (disease detection)\n- F1 Score: Balance between precision and recall\n- AUC-ROC: Overall ranking ability\n- Log Loss: Probability calibration\n\n**Regression:**\n- MAE: Robust to outliers, interpretable\n- MSE/RMSE: Penalizes large errors more\n- MAPE: Percentage-based, scale-independent\n- RÂ²: Explained variance ratio\n\n**Ranking:**\n- NDCG: Normalized discounted cumulative gain\n- MAP: Mean average precision\n- MRR: Mean reciprocal rank\n\n### Validation Strategies\n1. **Holdout**: Simple train/test split (quick, less reliable)\n2. **K-Fold CV**: K iterations, each fold as test once\n3. **Stratified K-Fold**: Preserves class distribution\n4. **Time Series CV**: Respects temporal order\n5. **Leave-One-Out**: Each sample as test once (small datasets)\n\n## Examples\n\n**Complete Evaluation Pipeline:**\n```python\nimport numpy as np\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, classification_report,\n    mean_absolute_error, mean_squared_error, r2_score\n)\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass ModelEvaluator:\n    def __init__(self, model, X_test, y_test, task='classification'):\n        self.model = model\n        self.X_test = X_test\n        self.y_test = y_test\n        self.task = task\n        self.y_pred = model.predict(X_test)\n        if task == 'classification' and hasattr(model, 'predict_proba'):\n            self.y_prob = model.predict_proba(X_test)[:, 1]\n    \n    def get_metrics(self):\n        if self.task == 'classification':\n            metrics = {\n                'accuracy': accuracy_score(self.y_test, self.y_pred),\n                'precision': precision_score(self.y_test, self.y_pred, average='weighted'),\n                'recall': recall_score(self.y_test, self.y_pred, average='weighted'),\n                'f1': f1_score(self.y_test, self.y_pred, average='weighted'),\n            }\n            if hasattr(self, 'y_prob'):\n                metrics['auc_roc'] = roc_auc_score(self.y_test, self.y_prob)\n            return metrics\n        else:\n            return {\n                'mae': mean_absolute_error(self.y_test, self.y_pred),\n                'rmse': np.sqrt(mean_squared_error(self.y_test, self.y_pred)),\n                'r2': r2_score(self.y_test, self.y_pred),\n                'mape': np.mean(np.abs((self.y_test - self.y_pred) / self.y_test)) * 100\n            }\n    \n    def plot_confusion_matrix(self):\n        cm = confusion_matrix(self.y_test, self.y_pred)\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n        plt.title('Confusion Matrix')\n        plt.ylabel('Actual')\n        plt.xlabel('Predicted')\n        plt.show()\n    \n    def cross_validate(self, X, y, cv=5):\n        if self.task == 'classification':\n            cv_strategy = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n            scoring = 'f1_weighted'\n        else:\n            cv_strategy = cv\n            scoring = 'neg_mean_squared_error'\n        \n        scores = cross_val_score(self.model, X, y, cv=cv_strategy, scoring=scoring)\n        return {\n            'mean': scores.mean(),\n            'std': scores.std(),\n            'scores': scores.tolist()\n        }\n    \n    def print_report(self):\n        print(\"=\" * 50)\n        print(\"MODEL EVALUATION REPORT\")\n        print(\"=\" * 50)\n        \n        metrics = self.get_metrics()\n        for name, value in metrics.items():\n            print(f\"{name.upper():15} {value:.4f}\")\n        \n        if self.task == 'classification':\n            print(\"\\nClassification Report:\")\n            print(classification_report(self.y_test, self.y_pred))\n```\n\n**Bias-Variance Analysis:**\n```python\ndef learning_curve_analysis(model, X, y, train_sizes=np.linspace(0.1, 1.0, 10)):\n    from sklearn.model_selection import learning_curve\n    \n    train_sizes, train_scores, val_scores = learning_curve(\n        model, X, y, train_sizes=train_sizes, cv=5,\n        scoring='accuracy', n_jobs=-1\n    )\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(train_sizes, train_scores.mean(axis=1), label='Training score')\n    plt.plot(train_sizes, val_scores.mean(axis=1), label='Validation score')\n    plt.fill_between(train_sizes, \n                     train_scores.mean(axis=1) - train_scores.std(axis=1),\n                     train_scores.mean(axis=1) + train_scores.std(axis=1), alpha=0.1)\n    plt.fill_between(train_sizes,\n                     val_scores.mean(axis=1) - val_scores.std(axis=1),\n                     val_scores.mean(axis=1) + val_scores.std(axis=1), alpha=0.1)\n    plt.xlabel('Training Size')\n    plt.ylabel('Score')\n    plt.title('Learning Curve (Bias-Variance Diagnosis)')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    \n    # Diagnosis\n    gap = train_scores.mean(axis=1)[-1] - val_scores.mean(axis=1)[-1]\n    if gap > 0.1:\n        print(\"Diagnosis: HIGH VARIANCE (Overfitting) - Try regularization or more data\")\n    elif val_scores.mean(axis=1)[-1] < 0.7:\n        print(\"Diagnosis: HIGH BIAS (Underfitting) - Try more features or complex model\")\n    else:\n        print(\"Diagnosis: Good fit\")\n```",
  "prompt": "A skill for evaluating ML models",
  "createdAt": "2024-01-15T10:00:00.000Z",
  "published": true
}
