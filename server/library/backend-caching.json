{
  "id": "backend-caching",
  "name": "Caching Strategies",
  "description": "Complete caching patterns with Redis, in-memory caching, cache invalidation, cache-aside, write-through, and performance optimization",
  "category": "Backend",
  "content": "---\nname: Caching Strategies\ndescription: Complete caching patterns with Redis, in-memory caching, cache invalidation, cache-aside, write-through, and performance optimization\ncategory: Backend\n---\n\n# Caching Strategies\n\n## Overview\n\nThis skill provides production-ready caching patterns using Redis and in-memory caches. It covers cache-aside, write-through, write-behind, cache invalidation strategies, TTL management, distributed caching, cache warming, and performance optimization techniques.\n\n## Activation\n\nUse this skill when:\n- Implementing caching to improve application performance\n- Setting up Redis for distributed caching\n- Adding in-memory caching with LRU eviction\n- Implementing cache invalidation strategies\n- Building cache-aside or write-through patterns\n- Adding cache warming for frequently accessed data\n- Implementing cache stampede prevention\n- Optimizing database query performance with caching\n- Building session storage or rate limiting\n\n## Instructions\n\n1. **Choose Cache Strategy**: Select cache-aside, write-through, or write-behind\n2. **Set up Redis**: Configure Redis connection and client\n3. **Define TTL**: Set appropriate time-to-live for cached data\n4. **Implement Serialization**: Handle JSON serialization/deserialization\n5. **Add Invalidation**: Implement cache invalidation on data changes\n6. **Handle Cache Misses**: Implement fallback to database\n7. **Prevent Stampede**: Use locking or probabilistic early expiration\n8. **Monitor Cache**: Track hit rates, miss rates, and memory usage\n9. **Use Cache Tags**: Group related cache entries for bulk invalidation\n10. **Optimize**: Use compression, pipelining, and batch operations\n\n## Examples\n\n### Redis Setup & Configuration\n\n```typescript\n// src/config/redis.config.ts\nimport Redis, { RedisOptions } from 'ioredis';\n\nconst redisConfig: RedisOptions = {\n  host: process.env.REDIS_HOST || 'localhost',\n  port: parseInt(process.env.REDIS_PORT || '6379'),\n  password: process.env.REDIS_PASSWORD,\n  db: parseInt(process.env.REDIS_DB || '0'),\n  retryStrategy: (times: number) => {\n    const delay = Math.min(times * 50, 2000);\n    return delay;\n  },\n  maxRetriesPerRequest: 3,\n  enableReadyCheck: true,\n  lazyConnect: true,\n};\n\nexport const redis = new Redis(redisConfig);\n\n// Pub/Sub client (separate connection)\nexport const redisPub = new Redis(redisConfig);\nexport const redisSub = new Redis(redisConfig);\n\n// Event handlers\nredis.on('connect', () => {\n  console.log('Redis connected');\n});\n\nredis.on('error', (err) => {\n  console.error('Redis error:', err);\n});\n\nredis.on('ready', () => {\n  console.log('Redis ready');\n});\n\n// Graceful shutdown\nprocess.on('SIGTERM', async () => {\n  await redis.quit();\n  await redisPub.quit();\n  await redisSub.quit();\n});\n\n// src/utils/cache.util.ts\nimport { redis } from '../config/redis.config';\nimport { compress, decompress } from 'lz-string';\n\nexport interface CacheOptions {\n  ttl?: number; // Time to live in seconds\n  compress?: boolean; // Compress large values\n  tags?: string[]; // Tags for grouped invalidation\n}\n\nexport class CacheUtil {\n  // Default TTL: 1 hour\n  private static DEFAULT_TTL = 3600;\n\n  // Serialize value\n  private static serialize(value: any, compress = false): string {\n    const json = JSON.stringify(value);\n    return compress ? compressString(json) : json;\n  }\n\n  // Deserialize value\n  private static deserialize(value: string | null, compressed = false): any {\n    if (!value) return null;\n    const json = compressed ? decompress(value) : value;\n    return json ? JSON.parse(json) : null;\n  }\n\n  // Generate cache key\n  static generateKey(...parts: string[]): string {\n    return parts.join(':');\n  }\n\n  // Set cache value\n  static async set(\n    key: string,\n    value: any,\n    options: CacheOptions = {}\n  ): Promise<void> {\n    const { ttl = this.DEFAULT_TTL, compress = false, tags = [] } = options;\n\n    const serialized = this.serialize(value, compress);\n\n    if (ttl > 0) {\n      await redis.setex(key, ttl, serialized);\n    } else {\n      await redis.set(key, serialized);\n    }\n\n    // Store tags for grouped invalidation\n    if (tags.length > 0) {\n      const tagPromises = tags.map(tag =>\n        redis.sadd(`tag:${tag}`, key)\n      );\n      await Promise.all(tagPromises);\n    }\n  }\n\n  // Get cache value\n  static async get<T = any>(\n    key: string,\n    compressed = false\n  ): Promise<T | null> {\n    const value = await redis.get(key);\n    return this.deserialize(value, compressed);\n  }\n\n  // Get multiple values\n  static async mget<T = any>(\n    keys: string[],\n    compressed = false\n  ): Promise<(T | null)[]> {\n    if (keys.length === 0) return [];\n    const values = await redis.mget(...keys);\n    return values.map(v => this.deserialize(v, compressed));\n  }\n\n  // Set multiple values\n  static async mset(\n    entries: Record<string, any>,\n    options: CacheOptions = {}\n  ): Promise<void> {\n    const { ttl = this.DEFAULT_TTL, compress = false } = options;\n\n    const pipeline = redis.pipeline();\n\n    for (const [key, value] of Object.entries(entries)) {\n      const serialized = this.serialize(value, compress);\n      if (ttl > 0) {\n        pipeline.setex(key, ttl, serialized);\n      } else {\n        pipeline.set(key, serialized);\n      }\n    }\n\n    await pipeline.exec();\n  }\n\n  // Delete cache key\n  static async del(key: string): Promise<void> {\n    await redis.del(key);\n  }\n\n  // Delete multiple keys\n  static async mdel(keys: string[]): Promise<void> {\n    if (keys.length === 0) return;\n    await redis.del(...keys);\n  }\n\n  // Invalidate by tag\n  static async invalidateByTag(tag: string): Promise<void> {\n    const keys = await redis.smembers(`tag:${tag}`);\n    if (keys.length > 0) {\n      await redis.del(...keys);\n    }\n    await redis.del(`tag:${tag}`);\n  }\n\n  // Check if key exists\n  static async exists(key: string): Promise<boolean> {\n    const result = await redis.exists(key);\n    return result === 1;\n  }\n\n  // Set expiration\n  static async expire(key: string, seconds: number): Promise<void> {\n    await redis.expire(key, seconds);\n  }\n\n  // Get TTL\n  static async ttl(key: string): Promise<number> {\n    return redis.ttl(key);\n  }\n\n  // Increment counter\n  static async incr(key: string, by = 1): Promise<number> {\n    return redis.incrby(key, by);\n  }\n\n  // Decrement counter\n  static async decr(key: string, by = 1): Promise<number> {\n    return redis.decrby(key, by);\n  }\n\n  // Clear all cache\n  static async flush(): Promise<void> {\n    await redis.flushdb();\n  }\n\n  // Get cache stats\n  static async stats() {\n    const info = await redis.info('stats');\n    const dbsize = await redis.dbsize();\n    \n    return {\n      dbsize,\n      info,\n    };\n  }\n}\n\n// src/services/cache.service.ts\nimport { CacheUtil } from '../utils/cache.util';\n\nexport class CacheService {\n  // Cache-aside pattern\n  static async cacheAside<T>(\n    key: string,\n    fetchFn: () => Promise<T>,\n    ttl = 3600\n  ): Promise<T> {\n    // Try to get from cache\n    const cached = await CacheUtil.get<T>(key);\n    if (cached !== null) {\n      return cached;\n    }\n\n    // Cache miss - fetch from source\n    const data = await fetchFn();\n\n    // Store in cache\n    await CacheUtil.set(key, data, { ttl });\n\n    return data;\n  }\n\n  // Cache-aside with lock (prevent cache stampede)\n  static async cacheAsideWithLock<T>(\n    key: string,\n    fetchFn: () => Promise<T>,\n    ttl = 3600,\n    lockTtl = 10\n  ): Promise<T> {\n    // Try to get from cache\n    const cached = await CacheUtil.get<T>(key);\n    if (cached !== null) {\n      return cached;\n    }\n\n    const lockKey = `lock:${key}`;\n\n    // Try to acquire lock\n    const locked = await redis.set(lockKey, '1', 'EX', lockTtl, 'NX');\n\n    if (locked === 'OK') {\n      try {\n        // We got the lock - fetch and cache\n        const data = await fetchFn();\n        await CacheUtil.set(key, data, { ttl });\n        return data;\n      } finally {\n        // Release lock\n        await redis.del(lockKey);\n      }\n    } else {\n      // Someone else is fetching - wait a bit and retry\n      await new Promise(resolve => setTimeout(resolve, 100));\n      return this.cacheAsideWithLock(key, fetchFn, ttl, lockTtl);\n    }\n  }\n\n  // Probabilistic early expiration (prevent stampede)\n  static async cacheAsideWithPEE<T>(\n    key: string,\n    fetchFn: () => Promise<T>,\n    ttl = 3600,\n    delta = 60,\n    beta = 1\n  ): Promise<T> {\n    const cached = await CacheUtil.get<T>(key);\n    const currentTtl = await CacheUtil.ttl(key);\n\n    // Calculate probabilistic early expiration\n    const shouldRefresh =\n      currentTtl > 0 &&\n      currentTtl < delta * beta * Math.log(Math.random());\n\n    if (cached !== null && !shouldRefresh) {\n      return cached;\n    }\n\n    // Fetch and update cache\n    const data = await fetchFn();\n    await CacheUtil.set(key, data, { ttl });\n\n    return data;\n  }\n\n  // Write-through pattern\n  static async writeThrough<T>(\n    key: string,\n    data: T,\n    saveFn: (data: T) => Promise<T>,\n    ttl = 3600\n  ): Promise<T> {\n    // Write to database\n    const saved = await saveFn(data);\n\n    // Write to cache\n    await CacheUtil.set(key, saved, { ttl });\n\n    return saved;\n  }\n\n  // Write-behind pattern (async)\n  static async writeBehind<T>(\n    key: string,\n    data: T,\n    saveFn: (data: T) => Promise<T>,\n    ttl = 3600\n  ): Promise<T> {\n    // Write to cache immediately\n    await CacheUtil.set(key, data, { ttl });\n\n    // Write to database asynchronously\n    saveFn(data).catch(err => {\n      console.error('Write-behind error:', err);\n      // Could add to a queue for retry\n    });\n\n    return data;\n  }\n\n  // Cache warming\n  static async warmCache<T>(\n    keys: string[],\n    fetchFn: (key: string) => Promise<T>,\n    ttl = 3600\n  ): Promise<void> {\n    const promises = keys.map(async key => {\n      const data = await fetchFn(key);\n      await CacheUtil.set(key, data, { ttl });\n    });\n\n    await Promise.all(promises);\n  }\n}\n\n// src/decorators/cache.decorator.ts\nimport { CacheService } from '../services/cache.service';\n\nexport function Cacheable(options: {\n  ttl?: number;\n  keyGenerator?: (...args: any[]) => string;\n}) {\n  return function (\n    target: any,\n    propertyKey: string,\n    descriptor: PropertyDescriptor\n  ) {\n    const originalMethod = descriptor.value;\n\n    descriptor.value = async function (...args: any[]) {\n      const key = options.keyGenerator\n        ? options.keyGenerator(...args)\n        : `${target.constructor.name}:${propertyKey}:${JSON.stringify(args)}`;\n\n      return CacheService.cacheAside(\n        key,\n        () => originalMethod.apply(this, args),\n        options.ttl\n      );\n    };\n\n    return descriptor;\n  };\n}\n\nexport function CacheInvalidate(options: {\n  keyGenerator?: (...args: any[]) => string | string[];\n}) {\n  return function (\n    target: any,\n    propertyKey: string,\n    descriptor: PropertyDescriptor\n  ) {\n    const originalMethod = descriptor.value;\n\n    descriptor.value = async function (...args: any[]) {\n      const result = await originalMethod.apply(this, args);\n\n      const keys = options.keyGenerator\n        ? options.keyGenerator(...args)\n        : [];\n\n      if (Array.isArray(keys)) {\n        await CacheUtil.mdel(keys);\n      } else {\n        await CacheUtil.del(keys);\n      }\n\n      return result;\n    };\n\n    return descriptor;\n  };\n}\n\n// src/repositories/user.repository.cached.ts\nimport { UserRepository } from './user.repository';\nimport { CacheUtil } from '../utils/cache.util';\nimport { CacheService } from '../services/cache.service';\nimport { User } from '../types';\n\nexport class CachedUserRepository extends UserRepository {\n  private static CACHE_PREFIX = 'user';\n  private static CACHE_TTL = 3600; // 1 hour\n\n  private static getCacheKey(id: string): string {\n    return CacheUtil.generateKey(this.CACHE_PREFIX, id);\n  }\n\n  private static getListCacheKey(page: number, limit: number): string {\n    return CacheUtil.generateKey(this.CACHE_PREFIX, 'list', `${page}`, `${limit}`);\n  }\n\n  // Get user with caching\n  static async findById(id: string): Promise<User | null> {\n    const cacheKey = this.getCacheKey(id);\n\n    return CacheService.cacheAsideWithLock(\n      cacheKey,\n      () => super.findById(id),\n      this.CACHE_TTL\n    );\n  }\n\n  // List users with caching\n  static async list(params: { skip: number; take: number }) {\n    const page = Math.floor(params.skip / params.take) + 1;\n    const cacheKey = this.getListCacheKey(page, params.take);\n\n    return CacheService.cacheAside(\n      cacheKey,\n      () => super.list(params),\n      300 // 5 minutes TTL for lists\n    );\n  }\n\n  // Create user and invalidate list cache\n  static async create(data: any): Promise<User> {\n    const user = await super.create(data);\n\n    // Cache the new user\n    const cacheKey = this.getCacheKey(user.id);\n    await CacheUtil.set(cacheKey, user, { ttl: this.CACHE_TTL });\n\n    // Invalidate list caches\n    await CacheUtil.invalidateByTag('user:list');\n\n    return user;\n  }\n\n  // Update user and invalidate cache\n  static async update(id: string, data: any): Promise<User> {\n    const user = await super.update(id, data);\n\n    // Update cache\n    const cacheKey = this.getCacheKey(id);\n    await CacheUtil.set(cacheKey, user, { ttl: this.CACHE_TTL });\n\n    // Invalidate list caches\n    await CacheUtil.invalidateByTag('user:list');\n\n    return user;\n  }\n\n  // Delete user and invalidate cache\n  static async delete(id: string): Promise<void> {\n    await super.delete(id);\n\n    // Delete from cache\n    const cacheKey = this.getCacheKey(id);\n    await CacheUtil.del(cacheKey);\n\n    // Invalidate list caches\n    await CacheUtil.invalidateByTag('user:list');\n  }\n}\n\n// src/middleware/cache.middleware.ts\nimport { Request, Response, NextFunction } from 'express';\nimport { CacheUtil } from '../utils/cache.util';\n\nexport const cacheMiddleware = (ttl = 60) => {\n  return async (req: Request, res: Response, next: NextFunction) => {\n    // Only cache GET requests\n    if (req.method !== 'GET') {\n      return next();\n    }\n\n    const cacheKey = `http:${req.originalUrl}`;\n\n    try {\n      // Check cache\n      const cached = await CacheUtil.get(cacheKey);\n\n      if (cached) {\n        return res.json(cached);\n      }\n\n      // Store original json method\n      const originalJson = res.json.bind(res);\n\n      // Override json method to cache response\n      res.json = (body: any) => {\n        // Cache the response\n        CacheUtil.set(cacheKey, body, { ttl }).catch(err => {\n          console.error('Cache set error:', err);\n        });\n\n        // Send response\n        return originalJson(body);\n      };\n\n      next();\n    } catch (error) {\n      console.error('Cache middleware error:', error);\n      next();\n    }\n  };\n};\n\n// src/utils/rate-limiter.ts\nimport { redis } from '../config/redis.config';\n\nexport class RateLimiter {\n  // Sliding window rate limiter\n  static async checkLimit(\n    key: string,\n    limit: number,\n    windowSeconds: number\n  ): Promise<{ allowed: boolean; remaining: number; resetAt: number }> {\n    const now = Date.now();\n    const windowStart = now - windowSeconds * 1000;\n\n    const pipeline = redis.pipeline();\n\n    // Remove old entries\n    pipeline.zremrangebyscore(key, 0, windowStart);\n\n    // Add current request\n    pipeline.zadd(key, now, `${now}`);\n\n    // Count requests in window\n    pipeline.zcard(key);\n\n    // Set expiration\n    pipeline.expire(key, windowSeconds);\n\n    const results = await pipeline.exec();\n    const count = results?.[2]?.[1] as number;\n\n    const allowed = count <= limit;\n    const remaining = Math.max(0, limit - count);\n    const resetAt = now + windowSeconds * 1000;\n\n    return { allowed, remaining, resetAt };\n  }\n\n  // Token bucket rate limiter\n  static async checkTokenBucket(\n    key: string,\n    capacity: number,\n    refillRate: number,\n    refillTime: number\n  ): Promise<boolean> {\n    const now = Date.now();\n    const bucketKey = `bucket:${key}`;\n\n    const bucket = await redis.get(bucketKey);\n    let tokens = capacity;\n    let lastRefill = now;\n\n    if (bucket) {\n      const data = JSON.parse(bucket);\n      tokens = data.tokens;\n      lastRefill = data.lastRefill;\n\n      // Refill tokens\n      const timePassed = now - lastRefill;\n      const refills = Math.floor(timePassed / refillTime);\n      tokens = Math.min(capacity, tokens + refills * refillRate);\n      lastRefill = lastRefill + refills * refillTime;\n    }\n\n    // Try to consume a token\n    if (tokens > 0) {\n      tokens--;\n      await redis.set(\n        bucketKey,\n        JSON.stringify({ tokens, lastRefill }),\n        'EX',\n        Math.ceil(capacity / refillRate) * refillTime\n      );\n      return true;\n    }\n\n    return false;\n  }\n}\n```\n\n## Best Practices\n\n1. **Set appropriate TTLs**: Balance freshness vs performance\n2. **Use cache tags**: Group related keys for easy invalidation\n3. **Prevent cache stampede**: Use locking or probabilistic expiration\n4. **Monitor cache hit rates**: Aim for 80%+ hit rate\n5. **Compress large values**: Save memory for objects >1KB\n6. **Use pipelines**: Batch multiple Redis operations\n7. **Handle cache failures gracefully**: Always have fallback to database\n8. **Invalidate on updates**: Keep cache and database in sync\n9. **Use separate cache instances**: Don't mix different data types\n10. **Version cache keys**: Include version in key for easy invalidation",
  "prompt": "A skill that provides complete caching strategies with Redis and in-memory caching, including cache-aside, write-through, cache invalidation, stampede prevention, and performance optimization with production-ready patterns",
  "createdAt": "2024-01-15T10:00:00.000Z",
  "published": true
}
