{
  "id": "ai-dataset-prep",
  "name": "dataset-preparation",
  "description": "Prepare and clean datasets for machine learning",
  "category": "AI",
  "content": "---\nname: dataset-preparation\ndescription: Prepare and clean datasets for ML\ncategory: AI\n---\n\n# Dataset Preparation\n\n## Overview\nPrepare, clean, and transform datasets for machine learning training and evaluation.\n\n## Activation\nActivates when user mentions \"dataset\", \"training data\", \"data cleaning\", \"data preparation\", \"feature engineering\", or \"data preprocessing\".\n\n## Instructions\n\n### Data Quality Checklist\n1. **Missing Values**: Identify and handle NaN/null values\n2. **Duplicates**: Remove or flag duplicate records\n3. **Outliers**: Detect and decide on outlier treatment\n4. **Data Types**: Ensure correct types for each column\n5. **Consistency**: Standardize formats (dates, categories)\n6. **Balance**: Check class distribution for classification tasks\n\n### Common Preprocessing Steps\n\n**Numerical Features:**\n- Normalization (0-1 scaling)\n- Standardization (z-score)\n- Log transformation for skewed data\n- Binning for discretization\n\n**Categorical Features:**\n- One-hot encoding\n- Label encoding\n- Target encoding\n- Handling rare categories\n\n**Text Data:**\n- Tokenization\n- Lowercasing and punctuation removal\n- Stop word removal\n- Stemming/Lemmatization\n- TF-IDF or embeddings\n\n**Time Series:**\n- Resampling to consistent intervals\n- Lag features\n- Rolling statistics\n- Seasonal decomposition\n\n## Examples\n\n**Complete Data Preprocessing Pipeline:**\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\ndef prepare_dataset(df, target_col, test_size=0.2):\n    \"\"\"\n    Complete dataset preparation pipeline.\n    \n    Args:\n        df: Raw pandas DataFrame\n        target_col: Name of target column\n        test_size: Fraction for test split\n    \n    Returns:\n        X_train, X_test, y_train, y_test, preprocessor_info\n    \"\"\"\n    df = df.copy()\n    \n    # 1. Remove duplicates\n    initial_rows = len(df)\n    df = df.drop_duplicates()\n    print(f\"Removed {initial_rows - len(df)} duplicates\")\n    \n    # 2. Handle missing values\n    missing_report = df.isnull().sum()\n    print(f\"Missing values:\\n{missing_report[missing_report > 0]}\")\n    \n    # Numeric: fill with median\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    for col in numeric_cols:\n        if df[col].isnull().any():\n            df[col].fillna(df[col].median(), inplace=True)\n    \n    # Categorical: fill with mode\n    cat_cols = df.select_dtypes(include=['object']).columns\n    for col in cat_cols:\n        if df[col].isnull().any():\n            df[col].fillna(df[col].mode()[0], inplace=True)\n    \n    # 3. Encode categorical variables\n    label_encoders = {}\n    for col in cat_cols:\n        if col != target_col:\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col].astype(str))\n            label_encoders[col] = le\n    \n    # 4. Handle outliers (IQR method)\n    for col in numeric_cols:\n        if col != target_col:\n            Q1, Q3 = df[col].quantile([0.25, 0.75])\n            IQR = Q3 - Q1\n            lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n            df[col] = df[col].clip(lower, upper)\n    \n    # 5. Split features and target\n    X = df.drop(columns=[target_col])\n    y = df[target_col]\n    \n    # 6. Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=42, stratify=y if y.dtype == 'object' else None\n    )\n    \n    # 7. Scale numeric features\n    scaler = StandardScaler()\n    X_train[numeric_cols.drop(target_col, errors='ignore')] = scaler.fit_transform(\n        X_train[numeric_cols.drop(target_col, errors='ignore')]\n    )\n    X_test[numeric_cols.drop(target_col, errors='ignore')] = scaler.transform(\n        X_test[numeric_cols.drop(target_col, errors='ignore')]\n    )\n    \n    preprocessor_info = {\n        'scaler': scaler,\n        'label_encoders': label_encoders,\n        'feature_names': X.columns.tolist()\n    }\n    \n    return X_train, X_test, y_train, y_test, preprocessor_info\n```\n\n**Data Quality Report:**\n```python\ndef generate_data_quality_report(df):\n    \"\"\"Generate comprehensive data quality report.\"\"\"\n    report = {\n        'shape': df.shape,\n        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n        'duplicates': df.duplicated().sum(),\n        'columns': {}\n    }\n    \n    for col in df.columns:\n        col_report = {\n            'dtype': str(df[col].dtype),\n            'missing': df[col].isnull().sum(),\n            'missing_pct': f\"{df[col].isnull().mean()*100:.1f}%\",\n            'unique': df[col].nunique(),\n            'sample_values': df[col].dropna().head(3).tolist()\n        }\n        \n        if df[col].dtype in ['int64', 'float64']:\n            col_report.update({\n                'min': df[col].min(),\n                'max': df[col].max(),\n                'mean': df[col].mean(),\n                'std': df[col].std()\n            })\n        \n        report['columns'][col] = col_report\n    \n    return report\n```",
  "prompt": "A skill for preparing ML datasets",
  "createdAt": "2024-01-15T10:00:00.000Z",
  "published": true
}
