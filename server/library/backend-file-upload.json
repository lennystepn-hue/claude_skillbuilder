{
  "id": "backend-file-upload",
  "name": "File Upload and Storage",
  "description": "Implement secure file uploads with Multer, S3, streaming, validation, image processing, and CDN integration",
  "category": "Backend",
  "content": "---\nname: File Upload and Storage\ndescription: Implement secure file uploads with Multer, S3, streaming, validation, image processing, and CDN integration\ncategory: Backend\n---\n\n# File Upload and Storage\n\n## Overview\n\nThis skill provides comprehensive patterns for implementing secure and efficient file upload functionality in backend applications. It covers Multer configuration, cloud storage integration (S3, Google Cloud Storage), file validation, image processing, streaming uploads, and security best practices.\n\n## Activation\n\nUse this skill when:\n- Implementing file upload functionality\n- Integrating with cloud storage (AWS S3, Google Cloud Storage, Azure Blob)\n- Processing uploaded images (resize, crop, optimize)\n- Handling large file uploads with streaming\n- Implementing file validation and security\n- Building file management APIs\n- Setting up CDN integration for file delivery\n- User mentions \"file upload\", \"multer\", \"S3\", \"image upload\", or \"file storage\"\n\n## Instructions\n\n1. **Configure File Upload Middleware**\n   - Use Multer for handling multipart/form-data\n   - Set appropriate file size limits\n   - Configure storage (memory, disk, or cloud)\n   - Implement proper error handling\n\n2. **Validate Files Thoroughly**\n   - Check file types and extensions\n   - Validate MIME types\n   - Scan file content (magic bytes)\n   - Set size limits per file type\n   - Implement virus scanning for production\n\n3. **Secure File Handling**\n   - Never trust client-provided filenames\n   - Generate unique filenames (UUIDs)\n   - Sanitize file paths\n   - Store files outside web root\n   - Implement access control\n\n4. **Use Cloud Storage**\n   - Store files in S3/GCS for scalability\n   - Implement signed URLs for secure access\n   - Use CDN for fast delivery\n   - Set appropriate retention policies\n   - Enable versioning for important files\n\n5. **Process Images Efficiently**\n   - Use Sharp for fast image processing\n   - Generate multiple sizes/thumbnails\n   - Optimize images for web\n   - Convert to efficient formats (WebP)\n   - Process asynchronously for large files\n\n6. **Handle Large Files**\n   - Use streaming for memory efficiency\n   - Implement chunked uploads\n   - Provide upload progress tracking\n   - Set appropriate timeouts\n   - Implement resumable uploads\n\n7. **Implement Proper Cleanup**\n   - Delete temporary files after processing\n   - Implement file retention policies\n   - Clean up failed uploads\n   - Monitor storage usage\n\n## Examples\n\n### Example 1: Multer Configuration with Validation\n\n```javascript\n// middleware/upload.js - Comprehensive Multer setup\nconst multer = require('multer');\nconst path = require('path');\nconst crypto = require('crypto');\nconst fs = require('fs').promises;\nconst { ValidationError } = require('../errors/application-errors');\n\n// File type configurations\nconst FILE_TYPES = {\n  image: {\n    mimeTypes: ['image/jpeg', 'image/png', 'image/gif', 'image/webp'],\n    extensions: ['.jpg', '.jpeg', '.png', '.gif', '.webp'],\n    maxSize: 5 * 1024 * 1024, // 5MB\n    magicBytes: {\n      'image/jpeg': ['ffd8ffe0', 'ffd8ffe1', 'ffd8ffe2'],\n      'image/png': ['89504e47'],\n      'image/gif': ['47494638']\n    }\n  },\n  document: {\n    mimeTypes: ['application/pdf', 'application/msword', 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'],\n    extensions: ['.pdf', '.doc', '.docx'],\n    maxSize: 10 * 1024 * 1024 // 10MB\n  },\n  video: {\n    mimeTypes: ['video/mp4', 'video/mpeg', 'video/quicktime'],\n    extensions: ['.mp4', '.mpeg', '.mov'],\n    maxSize: 100 * 1024 * 1024 // 100MB\n  }\n};\n\n// Disk storage configuration\nconst diskStorage = multer.diskStorage({\n  destination: async (req, file, cb) => {\n    const uploadDir = path.join(__dirname, '../uploads', req.uploadType || 'general');\n    \n    try {\n      await fs.mkdir(uploadDir, { recursive: true });\n      cb(null, uploadDir);\n    } catch (error) {\n      cb(error);\n    }\n  },\n  \n  filename: (req, file, cb) => {\n    // Generate unique filename\n    const uniqueSuffix = crypto.randomBytes(16).toString('hex');\n    const ext = path.extname(file.originalname).toLowerCase();\n    const filename = `${Date.now()}-${uniqueSuffix}${ext}`;\n    \n    cb(null, filename);\n  }\n});\n\n// Memory storage for cloud uploads\nconst memoryStorage = multer.memoryStorage();\n\n// File filter for validation\nfunction createFileFilter(allowedTypes) {\n  return (req, file, cb) => {\n    const fileType = FILE_TYPES[allowedTypes];\n    \n    if (!fileType) {\n      return cb(new ValidationError('Invalid file type configuration'));\n    }\n    \n    // Check MIME type\n    if (!fileType.mimeTypes.includes(file.mimetype)) {\n      return cb(new ValidationError(\n        `Invalid file type. Allowed types: ${fileType.mimeTypes.join(', ')}`\n      ));\n    }\n    \n    // Check extension\n    const ext = path.extname(file.originalname).toLowerCase();\n    if (!fileType.extensions.includes(ext)) {\n      return cb(new ValidationError(\n        `Invalid file extension. Allowed extensions: ${fileType.extensions.join(', ')}`\n      ));\n    }\n    \n    cb(null, true);\n  };\n}\n\n// Validate file content (magic bytes)\nasync function validateFileContent(file) {\n  const fileType = Object.keys(FILE_TYPES).find(type => \n    FILE_TYPES[type].mimeTypes.includes(file.mimetype)\n  );\n  \n  if (!fileType || !FILE_TYPES[fileType].magicBytes) {\n    return true; // Skip magic byte check if not configured\n  }\n  \n  const buffer = file.buffer || await fs.readFile(file.path);\n  const magicBytes = buffer.toString('hex', 0, 4);\n  \n  const validMagicBytes = FILE_TYPES[fileType].magicBytes[file.mimetype] || [];\n  const isValid = validMagicBytes.some(valid => magicBytes.startsWith(valid));\n  \n  if (!isValid) {\n    throw new ValidationError('File content does not match file type');\n  }\n  \n  return true;\n}\n\n// Create upload middleware\nfunction createUploadMiddleware(options = {}) {\n  const {\n    type = 'image',\n    storage = 'disk',\n    maxFiles = 1,\n    fieldName = 'file'\n  } = options;\n  \n  const fileType = FILE_TYPES[type];\n  if (!fileType) {\n    throw new Error(`Invalid file type: ${type}`);\n  }\n  \n  const uploadConfig = {\n    storage: storage === 'memory' ? memoryStorage : diskStorage,\n    fileFilter: createFileFilter(type),\n    limits: {\n      fileSize: fileType.maxSize,\n      files: maxFiles\n    }\n  };\n  \n  const upload = multer(uploadConfig);\n  const handler = maxFiles === 1 ? upload.single(fieldName) : upload.array(fieldName, maxFiles);\n  \n  // Wrap with additional validation\n  return async (req, res, next) => {\n    handler(req, res, async (err) => {\n      if (err instanceof multer.MulterError) {\n        if (err.code === 'LIMIT_FILE_SIZE') {\n          return next(new ValidationError(\n            `File size exceeds limit of ${fileType.maxSize / 1024 / 1024}MB`\n          ));\n        }\n        if (err.code === 'LIMIT_FILE_COUNT') {\n          return next(new ValidationError(\n            `Maximum ${maxFiles} files allowed`\n          ));\n        }\n        return next(new ValidationError(err.message));\n      }\n      \n      if (err) {\n        return next(err);\n      }\n      \n      // Validate file content for uploaded files\n      try {\n        if (req.file) {\n          await validateFileContent(req.file);\n        }\n        if (req.files && Array.isArray(req.files)) {\n          await Promise.all(req.files.map(file => validateFileContent(file)));\n        }\n        \n        next();\n      } catch (error) {\n        // Clean up uploaded files on validation error\n        if (req.file?.path) {\n          await fs.unlink(req.file.path).catch(() => {});\n        }\n        if (req.files) {\n          await Promise.all(\n            req.files.map(file => fs.unlink(file.path).catch(() => {}))\n          );\n        }\n        \n        next(error);\n      }\n    });\n  };\n}\n\n// Upload middleware variants\nconst uploadImage = createUploadMiddleware({ type: 'image', storage: 'memory' });\nconst uploadImages = createUploadMiddleware({ type: 'image', maxFiles: 10, storage: 'memory' });\nconst uploadDocument = createUploadMiddleware({ type: 'document' });\nconst uploadVideo = createUploadMiddleware({ type: 'video', storage: 'memory' });\n\nmodule.exports = {\n  uploadImage,\n  uploadImages,\n  uploadDocument,\n  uploadVideo,\n  createUploadMiddleware,\n  FILE_TYPES\n};\n```\n\n### Example 2: AWS S3 Integration\n\n```javascript\n// services/s3-service.js - Complete S3 upload service\nconst AWS = require('aws-sdk');\nconst stream = require('stream');\nconst crypto = require('crypto');\nconst path = require('path');\nconst logger = require('../logger');\n\nclass S3Service {\n  constructor() {\n    this.s3 = new AWS.S3({\n      accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n      secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,\n      region: process.env.AWS_REGION || 'us-east-1'\n    });\n    \n    this.bucket = process.env.AWS_S3_BUCKET;\n    this.cdnUrl = process.env.CDN_URL;\n  }\n  \n  // Upload file from buffer\n  async uploadFile(file, options = {}) {\n    const {\n      folder = 'uploads',\n      acl = 'private',\n      metadata = {},\n      contentType\n    } = options;\n    \n    const key = this.generateKey(file.originalname, folder);\n    \n    const params = {\n      Bucket: this.bucket,\n      Key: key,\n      Body: file.buffer,\n      ContentType: contentType || file.mimetype,\n      ACL: acl,\n      Metadata: {\n        originalname: file.originalname,\n        uploadedAt: new Date().toISOString(),\n        ...metadata\n      }\n    };\n    \n    try {\n      logger.info('Uploading file to S3', {\n        key,\n        size: file.size,\n        contentType: params.ContentType\n      });\n      \n      const result = await this.s3.upload(params).promise();\n      \n      logger.info('File uploaded successfully', {\n        key,\n        location: result.Location,\n        etag: result.ETag\n      });\n      \n      return {\n        key: result.Key,\n        location: result.Location,\n        bucket: result.Bucket,\n        etag: result.ETag,\n        url: this.getFileUrl(result.Key),\n        cdnUrl: this.getCdnUrl(result.Key)\n      };\n    } catch (error) {\n      logger.error('S3 upload failed', {\n        error: error.message,\n        key\n      });\n      throw error;\n    }\n  }\n  \n  // Upload multiple files\n  async uploadFiles(files, options = {}) {\n    return Promise.all(\n      files.map(file => this.uploadFile(file, options))\n    );\n  }\n  \n  // Upload with streaming for large files\n  async uploadStream(fileStream, filename, options = {}) {\n    const {\n      folder = 'uploads',\n      contentType = 'application/octet-stream',\n      acl = 'private'\n    } = options;\n    \n    const key = this.generateKey(filename, folder);\n    \n    const pass = new stream.PassThrough();\n    fileStream.pipe(pass);\n    \n    const params = {\n      Bucket: this.bucket,\n      Key: key,\n      Body: pass,\n      ContentType: contentType,\n      ACL: acl\n    };\n    \n    try {\n      const result = await this.s3.upload(params).promise();\n      \n      return {\n        key: result.Key,\n        location: result.Location,\n        url: this.getFileUrl(result.Key)\n      };\n    } catch (error) {\n      logger.error('S3 stream upload failed', {\n        error: error.message,\n        key\n      });\n      throw error;\n    }\n  }\n  \n  // Get file from S3\n  async getFile(key) {\n    const params = {\n      Bucket: this.bucket,\n      Key: key\n    };\n    \n    try {\n      const result = await this.s3.getObject(params).promise();\n      return result.Body;\n    } catch (error) {\n      logger.error('S3 get file failed', {\n        error: error.message,\n        key\n      });\n      throw error;\n    }\n  }\n  \n  // Get file stream\n  getFileStream(key) {\n    const params = {\n      Bucket: this.bucket,\n      Key: key\n    };\n    \n    return this.s3.getObject(params).createReadStream();\n  }\n  \n  // Generate signed URL for temporary access\n  getSignedUrl(key, expiresIn = 3600) {\n    const params = {\n      Bucket: this.bucket,\n      Key: key,\n      Expires: expiresIn\n    };\n    \n    return this.s3.getSignedUrl('getObject', params);\n  }\n  \n  // Generate signed URL for upload\n  getSignedUploadUrl(key, contentType, expiresIn = 3600) {\n    const params = {\n      Bucket: this.bucket,\n      Key: key,\n      ContentType: contentType,\n      Expires: expiresIn\n    };\n    \n    return this.s3.getSignedUrl('putObject', params);\n  }\n  \n  // Delete file\n  async deleteFile(key) {\n    const params = {\n      Bucket: this.bucket,\n      Key: key\n    };\n    \n    try {\n      await this.s3.deleteObject(params).promise();\n      logger.info('File deleted from S3', { key });\n      return true;\n    } catch (error) {\n      logger.error('S3 delete failed', {\n        error: error.message,\n        key\n      });\n      throw error;\n    }\n  }\n  \n  // Delete multiple files\n  async deleteFiles(keys) {\n    const params = {\n      Bucket: this.bucket,\n      Delete: {\n        Objects: keys.map(key => ({ Key: key }))\n      }\n    };\n    \n    try {\n      const result = await this.s3.deleteObjects(params).promise();\n      logger.info('Files deleted from S3', {\n        count: result.Deleted.length\n      });\n      return result;\n    } catch (error) {\n      logger.error('S3 bulk delete failed', {\n        error: error.message,\n        count: keys.length\n      });\n      throw error;\n    }\n  }\n  \n  // Check if file exists\n  async fileExists(key) {\n    const params = {\n      Bucket: this.bucket,\n      Key: key\n    };\n    \n    try {\n      await this.s3.headObject(params).promise();\n      return true;\n    } catch (error) {\n      if (error.code === 'NotFound') {\n        return false;\n      }\n      throw error;\n    }\n  }\n  \n  // Copy file\n  async copyFile(sourceKey, destinationKey) {\n    const params = {\n      Bucket: this.bucket,\n      CopySource: `${this.bucket}/${sourceKey}`,\n      Key: destinationKey\n    };\n    \n    try {\n      await this.s3.copyObject(params).promise();\n      logger.info('File copied in S3', {\n        from: sourceKey,\n        to: destinationKey\n      });\n      return true;\n    } catch (error) {\n      logger.error('S3 copy failed', {\n        error: error.message,\n        from: sourceKey,\n        to: destinationKey\n      });\n      throw error;\n    }\n  }\n  \n  // Helper: Generate unique key\n  generateKey(filename, folder = 'uploads') {\n    const ext = path.extname(filename);\n    const basename = path.basename(filename, ext);\n    const hash = crypto.randomBytes(8).toString('hex');\n    const timestamp = Date.now();\n    \n    return `${folder}/${timestamp}-${hash}${ext}`;\n  }\n  \n  // Helper: Get file URL\n  getFileUrl(key) {\n    return `https://${this.bucket}.s3.${this.s3.config.region}.amazonaws.com/${key}`;\n  }\n  \n  // Helper: Get CDN URL\n  getCdnUrl(key) {\n    if (this.cdnUrl) {\n      return `${this.cdnUrl}/${key}`;\n    }\n    return this.getFileUrl(key);\n  }\n}\n\nmodule.exports = new S3Service();\n```\n\n### Example 3: Image Processing with Sharp\n\n```javascript\n// services/image-service.js - Image processing and optimization\nconst sharp = require('sharp');\nconst path = require('path');\nconst logger = require('../logger');\nconst s3Service = require('./s3-service');\n\nclass ImageService {\n  constructor() {\n    this.sizes = {\n      thumbnail: { width: 150, height: 150, fit: 'cover' },\n      small: { width: 300, height: 300, fit: 'inside' },\n      medium: { width: 800, height: 800, fit: 'inside' },\n      large: { width: 1920, height: 1920, fit: 'inside' }\n    };\n  }\n  \n  // Process and upload image with multiple sizes\n  async processAndUpload(file, options = {}) {\n    const {\n      generateSizes = ['thumbnail', 'medium'],\n      format = 'webp',\n      quality = 80,\n      folder = 'images'\n    } = options;\n    \n    try {\n      logger.info('Processing image', {\n        filename: file.originalname,\n        size: file.size,\n        sizes: generateSizes\n      });\n      \n      const results = {};\n      const metadata = await sharp(file.buffer).metadata();\n      \n      // Process original image\n      const originalBuffer = await sharp(file.buffer)\n        .rotate() // Auto-rotate based on EXIF\n        .toFormat(format, { quality })\n        .toBuffer();\n      \n      results.original = await s3Service.uploadFile(\n        {\n          ...file,\n          buffer: originalBuffer,\n          originalname: this.changeExtension(file.originalname, format)\n        },\n        {\n          folder,\n          metadata: {\n            width: metadata.width.toString(),\n            height: metadata.height.toString(),\n            format: metadata.format\n          }\n        }\n      );\n      \n      // Generate and upload different sizes\n      for (const sizeName of generateSizes) {\n        const sizeConfig = this.sizes[sizeName];\n        if (!sizeConfig) continue;\n        \n        const resizedBuffer = await sharp(file.buffer)\n          .rotate()\n          .resize(sizeConfig.width, sizeConfig.height, {\n            fit: sizeConfig.fit,\n            withoutEnlargement: true\n          })\n          .toFormat(format, { quality })\n          .toBuffer();\n        \n        results[sizeName] = await s3Service.uploadFile(\n          {\n            ...file,\n            buffer: resizedBuffer,\n            originalname: this.addSuffix(\n              this.changeExtension(file.originalname, format),\n              sizeName\n            )\n          },\n          { folder: `${folder}/${sizeName}` }\n        );\n      }\n      \n      logger.info('Image processed successfully', {\n        filename: file.originalname,\n        sizes: Object.keys(results)\n      });\n      \n      return results;\n    } catch (error) {\n      logger.error('Image processing failed', {\n        error: error.message,\n        filename: file.originalname\n      });\n      throw error;\n    }\n  }\n  \n  // Optimize image\n  async optimizeImage(buffer, options = {}) {\n    const {\n      format = 'webp',\n      quality = 80,\n      maxWidth = 1920,\n      maxHeight = 1920\n    } = options;\n    \n    return sharp(buffer)\n      .rotate()\n      .resize(maxWidth, maxHeight, {\n        fit: 'inside',\n        withoutEnlargement: true\n      })\n      .toFormat(format, { quality })\n      .toBuffer();\n  }\n  \n  // Create thumbnail\n  async createThumbnail(buffer, width = 150, height = 150) {\n    return sharp(buffer)\n      .resize(width, height, {\n        fit: 'cover',\n        position: 'center'\n      })\n      .toFormat('jpeg', { quality: 80 })\n      .toBuffer();\n  }\n  \n  // Convert image format\n  async convertFormat(buffer, format = 'webp', quality = 80) {\n    return sharp(buffer)\n      .toFormat(format, { quality })\n      .toBuffer();\n  }\n  \n  // Add watermark\n  async addWatermark(buffer, watermarkPath, options = {}) {\n    const {\n      position = 'southeast',\n      opacity = 0.5\n    } = options;\n    \n    const watermark = await sharp(watermarkPath)\n      .ensureAlpha()\n      .modulate({ brightness: 1, saturation: 1 })\n      .toBuffer();\n    \n    return sharp(buffer)\n      .composite([{\n        input: watermark,\n        gravity: position,\n        blend: 'over'\n      }])\n      .toBuffer();\n  }\n  \n  // Get image metadata\n  async getMetadata(buffer) {\n    return sharp(buffer).metadata();\n  }\n  \n  // Crop image\n  async cropImage(buffer, { x, y, width, height }) {\n    return sharp(buffer)\n      .extract({ left: x, top: y, width, height })\n      .toBuffer();\n  }\n  \n  // Helper: Change file extension\n  changeExtension(filename, newExt) {\n    const ext = path.extname(filename);\n    const basename = path.basename(filename, ext);\n    return `${basename}.${newExt}`;\n  }\n  \n  // Helper: Add suffix to filename\n  addSuffix(filename, suffix) {\n    const ext = path.extname(filename);\n    const basename = path.basename(filename, ext);\n    return `${basename}-${suffix}${ext}`;\n  }\n}\n\nmodule.exports = new ImageService();\n```\n\n### Example 4: File Upload Controller\n\n```javascript\n// controllers/upload-controller.js - Complete upload endpoints\nconst imageService = require('../services/image-service');\nconst s3Service = require('../services/s3-service');\nconst logger = require('../logger');\nconst { BadRequestError } = require('../errors/application-errors');\n\nclass UploadController {\n  // Upload single image\n  async uploadImage(req, res, next) {\n    try {\n      if (!req.file) {\n        throw new BadRequestError('No file uploaded');\n      }\n      \n      logger.info('Image upload request', {\n        filename: req.file.originalname,\n        size: req.file.size,\n        mimetype: req.file.mimetype,\n        userId: req.user?.id\n      });\n      \n      // Process and upload image\n      const results = await imageService.processAndUpload(req.file, {\n        generateSizes: ['thumbnail', 'small', 'medium'],\n        format: 'webp',\n        quality: 85,\n        folder: 'user-uploads'\n      });\n      \n      res.json({\n        success: true,\n        data: {\n          original: results.original.url,\n          thumbnail: results.thumbnail?.url,\n          small: results.small?.url,\n          medium: results.medium?.url,\n          metadata: {\n            key: results.original.key,\n            filename: req.file.originalname,\n            size: req.file.size,\n            mimetype: req.file.mimetype\n          }\n        }\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n  \n  // Upload multiple images\n  async uploadImages(req, res, next) {\n    try {\n      if (!req.files || req.files.length === 0) {\n        throw new BadRequestError('No files uploaded');\n      }\n      \n      logger.info('Multiple images upload request', {\n        count: req.files.length,\n        userId: req.user?.id\n      });\n      \n      // Process all images in parallel\n      const results = await Promise.all(\n        req.files.map(file => \n          imageService.processAndUpload(file, {\n            generateSizes: ['thumbnail', 'medium'],\n            folder: 'user-uploads'\n          })\n        )\n      );\n      \n      res.json({\n        success: true,\n        data: results.map((result, index) => ({\n          original: result.original.url,\n          thumbnail: result.thumbnail?.url,\n          medium: result.medium?.url,\n          filename: req.files[index].originalname\n        }))\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n  \n  // Upload document\n  async uploadDocument(req, res, next) {\n    try {\n      if (!req.file) {\n        throw new BadRequestError('No file uploaded');\n      }\n      \n      const result = await s3Service.uploadFile(req.file, {\n        folder: 'documents',\n        acl: 'private',\n        metadata: {\n          userId: req.user?.id,\n          uploadedBy: req.user?.email\n        }\n      });\n      \n      res.json({\n        success: true,\n        data: {\n          key: result.key,\n          filename: req.file.originalname,\n          size: req.file.size,\n          url: result.url\n        }\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n  \n  // Get signed URL for direct upload\n  async getUploadUrl(req, res, next) {\n    try {\n      const { filename, contentType } = req.body;\n      \n      if (!filename || !contentType) {\n        throw new BadRequestError('Filename and content type required');\n      }\n      \n      const key = s3Service.generateKey(filename, 'user-uploads');\n      const uploadUrl = s3Service.getSignedUploadUrl(key, contentType, 3600);\n      \n      res.json({\n        success: true,\n        data: {\n          uploadUrl,\n          key,\n          expiresIn: 3600\n        }\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n  \n  // Get signed URL for file download\n  async getDownloadUrl(req, res, next) {\n    try {\n      const { key } = req.params;\n      \n      // Check if file exists\n      const exists = await s3Service.fileExists(key);\n      if (!exists) {\n        throw new NotFoundError('File', key);\n      }\n      \n      const downloadUrl = s3Service.getSignedUrl(key, 3600);\n      \n      res.json({\n        success: true,\n        data: {\n          downloadUrl,\n          expiresIn: 3600\n        }\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n  \n  // Delete file\n  async deleteFile(req, res, next) {\n    try {\n      const { key } = req.params;\n      \n      await s3Service.deleteFile(key);\n      \n      res.json({\n        success: true,\n        message: 'File deleted successfully'\n      });\n    } catch (error) {\n      next(error);\n    }\n  }\n}\n\nmodule.exports = new UploadController();\n```\n\n### Example 5: Complete Upload Routes\n\n```javascript\n// routes/upload-routes.js - Upload API routes\nconst express = require('express');\nconst router = express.Router();\nconst uploadController = require('../controllers/upload-controller');\nconst { uploadImage, uploadImages, uploadDocument } = require('../middleware/upload');\nconst { authenticate } = require('../middleware/auth');\nconst { asyncHandler } = require('../middleware/error-handler');\n\n// All routes require authentication\nrouter.use(authenticate);\n\n// Image upload routes\nrouter.post(\n  '/image',\n  uploadImage,\n  asyncHandler(uploadController.uploadImage.bind(uploadController))\n);\n\nrouter.post(\n  '/images',\n  uploadImages,\n  asyncHandler(uploadController.uploadImages.bind(uploadController))\n);\n\n// Document upload\nrouter.post(\n  '/document',\n  uploadDocument,\n  asyncHandler(uploadController.uploadDocument.bind(uploadController))\n);\n\n// Get signed URLs\nrouter.post(\n  '/upload-url',\n  asyncHandler(uploadController.getUploadUrl.bind(uploadController))\n);\n\nrouter.get(\n  '/download-url/:key',\n  asyncHandler(uploadController.getDownloadUrl.bind(uploadController))\n);\n\n// Delete file\nrouter.delete(\n  '/:key',\n  asyncHandler(uploadController.deleteFile.bind(uploadController))\n);\n\nmodule.exports = router;\n\n// app.js - Register routes\nconst uploadRoutes = require('./routes/upload-routes');\napp.use('/api/upload', uploadRoutes);\n```",
  "prompt": "A skill that implements comprehensive file upload functionality including Multer configuration with validation, AWS S3 integration with streaming support, image processing with Sharp (resize, optimize, format conversion, watermarking), secure file handling, signed URLs, and complete upload/download API endpoints.",
  "createdAt": "2024-01-15T10:00:00.000Z",
  "published": true
}
