{
  "id": "ai-streaming",
  "name": "streaming-responses",
  "description": "Implement streaming LLM responses for better UX",
  "category": "AI",
  "content": "---\nname: streaming-responses\ndescription: Stream LLM responses for better UX\ncategory: AI\n---\n\n# Streaming Responses\n\n## Overview\nImplement streaming responses from LLMs for better user experience with real-time output.\n\n## Activation\nActivates when user mentions \"streaming\", \"stream response\", \"real-time output\", \"SSE\", \"server-sent events\", or \"progressive rendering\".\n\n## Instructions\n\n### Why Stream?\n- Better perceived performance\n- Users see output immediately\n- Can cancel mid-generation\n- Required for long responses\n- Better chat experience\n\n## Examples\n\n**OpenAI Streaming:**\n```python\nfrom openai import OpenAI\n\ndef stream_chat(prompt: str):\n    client = OpenAI()\n    \n    stream = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        stream=True\n    )\n    \n    full_response = \"\"\n    for chunk in stream:\n        if chunk.choices[0].delta.content:\n            content = chunk.choices[0].delta.content\n            full_response += content\n            print(content, end=\"\", flush=True)\n    \n    print()  # Newline at end\n    return full_response\n\n# Usage\nresponse = stream_chat(\"Explain quantum computing\")\n```\n\n**Anthropic Claude Streaming:**\n```python\nimport anthropic\n\ndef stream_claude(prompt: str):\n    client = anthropic.Anthropic()\n    \n    with client.messages.stream(\n        model=\"claude-sonnet-4-20250514\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    ) as stream:\n        full_response = \"\"\n        for text in stream.text_stream:\n            full_response += text\n            print(text, end=\"\", flush=True)\n    \n    print()\n    return full_response\n```\n\n**FastAPI SSE Endpoint:**\n```python\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom openai import OpenAI\nimport json\n\napp = FastAPI()\nclient = OpenAI()\n\nasync def generate_stream(prompt: str):\n    stream = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        stream=True\n    )\n    \n    for chunk in stream:\n        if chunk.choices[0].delta.content:\n            data = {\"content\": chunk.choices[0].delta.content}\n            yield f\"data: {json.dumps(data)}\\n\\n\"\n    \n    yield \"data: [DONE]\\n\\n\"\n\n@app.post(\"/chat/stream\")\nasync def chat_stream(prompt: str):\n    return StreamingResponse(\n        generate_stream(prompt),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\"\n        }\n    )\n```\n\n**React Frontend Consumer:**\n```typescript\nasync function streamChat(prompt: string, onChunk: (text: string) => void) {\n  const response = await fetch('/api/chat/stream', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ prompt })\n  });\n\n  const reader = response.body?.getReader();\n  const decoder = new TextDecoder();\n\n  while (reader) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n');\n\n    for (const line of lines) {\n      if (line.startsWith('data: ')) {\n        const data = line.slice(6);\n        if (data === '[DONE]') return;\n        \n        try {\n          const parsed = JSON.parse(data);\n          onChunk(parsed.content);\n        } catch (e) {}\n      }\n    }\n  }\n}\n\n// Usage in React component\nfunction ChatComponent() {\n  const [response, setResponse] = useState('');\n\n  const handleSend = async (prompt: string) => {\n    setResponse('');\n    await streamChat(prompt, (chunk) => {\n      setResponse(prev => prev + chunk);\n    });\n  };\n\n  return <div>{response}</div>;\n}\n```\n\n**Async Streaming with Progress:**\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nasync def stream_with_progress(prompt: str, on_progress: callable = None):\n    client = AsyncOpenAI()\n    \n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        stream=True\n    )\n    \n    full_response = \"\"\n    token_count = 0\n    \n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            content = chunk.choices[0].delta.content\n            full_response += content\n            token_count += 1\n            \n            if on_progress:\n                on_progress({\n                    \"content\": content,\n                    \"total\": full_response,\n                    \"tokens\": token_count\n                })\n    \n    return full_response\n\n# Usage\nasync def main():\n    def progress(data):\n        print(f\"[{data['tokens']} tokens] {data['content']}\", end=\"\")\n    \n    result = await stream_with_progress(\"Tell me a story\", progress)\n\nasyncio.run(main())\n```",
  "prompt": "A skill for streaming LLM responses",
  "createdAt": "2024-01-15T10:00:00.000Z",
  "published": true
}
