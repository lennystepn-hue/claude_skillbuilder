{
  "id": "ai-evaluation",
  "name": "llm-evaluation",
  "description": "Evaluate and benchmark LLM outputs systematically",
  "category": "AI",
  "content": "---\nname: llm-evaluation\ndescription: Evaluate LLM outputs systematically\ncategory: AI\n---\n\n# LLM Evaluation\n\n## Overview\nSystematically evaluate LLM outputs using automated metrics, human evaluation, and benchmarking.\n\n## Activation\nActivates when user mentions \"evaluate LLM\", \"benchmark\", \"LLM testing\", \"output quality\", \"hallucination detection\", or \"response evaluation\".\n\n## Instructions\n\n### Evaluation Dimensions\n1. **Accuracy**: Factual correctness\n2. **Relevance**: Addresses the prompt\n3. **Coherence**: Logical flow\n4. **Completeness**: Covers all aspects\n5. **Safety**: No harmful content\n6. **Groundedness**: Based on provided context\n\n### Evaluation Methods\n- **LLM-as-Judge**: Use another LLM to evaluate\n- **Reference Comparison**: Compare to gold standard\n- **Automated Metrics**: BLEU, ROUGE, BERTScore\n- **Human Evaluation**: Expert assessment\n\n## Examples\n\n**LLM-as-Judge Evaluator:**\n```python\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List\nimport json\n\nclass EvaluationResult(BaseModel):\n    relevance: int = Field(ge=1, le=5, description=\"How relevant is the response\")\n    accuracy: int = Field(ge=1, le=5, description=\"How accurate is the information\")\n    completeness: int = Field(ge=1, le=5, description=\"How complete is the answer\")\n    coherence: int = Field(ge=1, le=5, description=\"How well-structured and logical\")\n    reasoning: str = Field(description=\"Explanation for the scores\")\n    \n    @property\n    def overall(self) -> float:\n        return (self.relevance + self.accuracy + self.completeness + self.coherence) / 4\n\nclass LLMEvaluator:\n    def __init__(self, judge_model=\"gpt-4o\"):\n        self.client = OpenAI()\n        self.judge_model = judge_model\n    \n    def evaluate(self, prompt: str, response: str, context: str = None) -> EvaluationResult:\n        eval_prompt = f\"\"\"Evaluate this LLM response.\n\nOriginal Prompt: {prompt}\n{f\"Context Provided: {context}\" if context else \"\"}\n\nResponse to Evaluate:\n{response}\n\nRate each dimension from 1-5 (1=poor, 5=excellent):\n- Relevance: Does it address the prompt?\n- Accuracy: Is the information correct?\n- Completeness: Does it fully answer the question?\n- Coherence: Is it well-structured and logical?\n\nRespond with JSON:\n{{\"relevance\": X, \"accuracy\": X, \"completeness\": X, \"coherence\": X, \"reasoning\": \"...\"}}\"\"\"\n        \n        result = self.client.chat.completions.create(\n            model=self.judge_model,\n            messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n            response_format={\"type\": \"json_object\"}\n        )\n        \n        return EvaluationResult(**json.loads(result.choices[0].message.content))\n    \n    def compare(self, prompt: str, responses: List[str]) -> List[tuple[int, EvaluationResult]]:\n        results = [(i, self.evaluate(prompt, r)) for i, r in enumerate(responses)]\n        return sorted(results, key=lambda x: x[1].overall, reverse=True)\n\n# Usage\nevaluator = LLMEvaluator()\nresult = evaluator.evaluate(\n    prompt=\"Explain recursion in programming\",\n    response=\"Recursion is when a function calls itself...\"\n)\nprint(f\"Overall Score: {result.overall}/5\")\nprint(f\"Reasoning: {result.reasoning}\")\n```\n\n**Hallucination Detection:**\n```python\nclass HallucinationDetector:\n    def __init__(self):\n        self.client = OpenAI()\n    \n    def check_grounded(self, context: str, response: str) -> dict:\n        prompt = f\"\"\"Check if the response is grounded in the provided context.\n\nContext:\n{context}\n\nResponse:\n{response}\n\nFor each claim in the response, determine if it is:\n- SUPPORTED: Directly stated or implied in context\n- NOT_SUPPORTED: Cannot be verified from context\n- CONTRADICTED: Contradicts the context\n\nRespond with JSON:\n{{\n  \"claims\": [\n    {{\"claim\": \"...\", \"status\": \"SUPPORTED|NOT_SUPPORTED|CONTRADICTED\", \"evidence\": \"...\" }}\n  ],\n  \"hallucination_score\": 0.0-1.0,\n  \"summary\": \"...\"\n}}\"\"\"\n        \n        result = self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            response_format={\"type\": \"json_object\"}\n        )\n        \n        return json.loads(result.choices[0].message.content)\n\n# Usage\ndetector = HallucinationDetector()\nresult = detector.check_grounded(\n    context=\"Python was created by Guido van Rossum in 1991.\",\n    response=\"Python was created by Guido van Rossum in 1991. It's the most popular language.\"\n)\nprint(f\"Hallucination Score: {result['hallucination_score']}\")\n```\n\n**Benchmark Suite:**\n```python\nfrom dataclasses import dataclass\nfrom typing import Callable\nimport time\n\n@dataclass\nclass TestCase:\n    name: str\n    prompt: str\n    expected: str = None\n    validator: Callable[[str], bool] = None\n\nclass LLMBenchmark:\n    def __init__(self, model: str):\n        self.client = OpenAI()\n        self.model = model\n        self.results = []\n    \n    def run(self, test_cases: List[TestCase]) -> dict:\n        for tc in test_cases:\n            start = time.time()\n            \n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=[{\"role\": \"user\", \"content\": tc.prompt}]\n            )\n            \n            latency = time.time() - start\n            output = response.choices[0].message.content\n            tokens = response.usage.total_tokens\n            \n            passed = True\n            if tc.validator:\n                passed = tc.validator(output)\n            elif tc.expected:\n                passed = tc.expected.lower() in output.lower()\n            \n            self.results.append({\n                \"name\": tc.name,\n                \"passed\": passed,\n                \"latency\": latency,\n                \"tokens\": tokens,\n                \"output\": output[:200]\n            })\n        \n        return {\n            \"model\": self.model,\n            \"total\": len(test_cases),\n            \"passed\": sum(1 for r in self.results if r[\"passed\"]),\n            \"avg_latency\": sum(r[\"latency\"] for r in self.results) / len(self.results),\n            \"results\": self.results\n        }\n\n# Usage\nbenchmark = LLMBenchmark(\"gpt-4o\")\nresults = benchmark.run([\n    TestCase(\"math\", \"What is 15 * 17?\", expected=\"255\"),\n    TestCase(\"code\", \"Write a Python hello world\", validator=lambda x: \"print\" in x),\n])\nprint(f\"Passed: {results['passed']}/{results['total']}\")\n```",
  "prompt": "A skill for evaluating LLM outputs",
  "createdAt": "2024-01-15T10:00:00.000Z",
  "published": true
}
