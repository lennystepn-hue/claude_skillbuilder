{
  "id": "ai-vision",
  "name": "vision-ai",
  "description": "Process images with vision language models",
  "category": "AI",
  "content": "---\nname: vision-ai\ndescription: Image processing with vision LLMs\ncategory: AI\n---\n\n# Vision AI\n\n## Overview\nProcess and analyze images using vision-language models like GPT-4V and Claude Vision.\n\n## Activation\nActivates when user mentions \"vision\", \"image analysis\", \"GPT-4V\", \"Claude Vision\", \"image to text\", \"OCR\", or \"visual understanding\".\n\n## Instructions\n\n### Use Cases\n- Image description and captioning\n- OCR and document extraction\n- Visual Q&A\n- Object detection descriptions\n- UI/screenshot analysis\n- Chart and diagram interpretation\n\n### Best Practices\n- Resize large images to reduce tokens\n- Use detail parameter appropriately\n- Be specific in your prompts\n- Handle multiple images when needed\n\n## Examples\n\n**OpenAI GPT-4 Vision:**\n```python\nfrom openai import OpenAI\nimport base64\nfrom pathlib import Path\n\ndef encode_image(image_path: str) -> str:\n    with open(image_path, \"rb\") as f:\n        return base64.b64encode(f.read()).decode()\n\ndef analyze_image(image_path: str, prompt: str, detail: str = \"auto\") -> str:\n    client = OpenAI()\n    \n    base64_image = encode_image(image_path)\n    ext = Path(image_path).suffix.lower().replace(\".\", \"\")\n    media_type = f\"image/{'jpeg' if ext in ['jpg', 'jpeg'] else ext}\"\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": prompt},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:{media_type};base64,{base64_image}\",\n                        \"detail\": detail  # \"low\", \"high\", or \"auto\"\n                    }\n                }\n            ]\n        }],\n        max_tokens=1000\n    )\n    \n    return response.choices[0].message.content\n\n# Usage\ndescription = analyze_image(\"screenshot.png\", \"Describe what you see in this UI\")\nprint(description)\n```\n\n**Claude Vision:**\n```python\nimport anthropic\nimport base64\n\ndef analyze_with_claude(image_path: str, prompt: str) -> str:\n    client = anthropic.Anthropic()\n    \n    with open(image_path, \"rb\") as f:\n        image_data = base64.standard_b64encode(f.read()).decode()\n    \n    # Determine media type\n    ext = image_path.lower().split(\".\")[-1]\n    media_types = {\"jpg\": \"image/jpeg\", \"jpeg\": \"image/jpeg\", \"png\": \"image/png\", \"gif\": \"image/gif\", \"webp\": \"image/webp\"}\n    media_type = media_types.get(ext, \"image/jpeg\")\n    \n    response = client.messages.create(\n        model=\"claude-sonnet-4-20250514\",\n        max_tokens=1024,\n        messages=[{\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": media_type,\n                        \"data\": image_data\n                    }\n                },\n                {\"type\": \"text\", \"text\": prompt}\n            ]\n        }]\n    )\n    \n    return response.content[0].text\n\n# Usage\nresult = analyze_with_claude(\"diagram.png\", \"Explain this architecture diagram\")\n```\n\n**OCR and Document Extraction:**\n```python\ndef extract_document_data(image_path: str) -> dict:\n    client = OpenAI()\n    \n    prompt = \"\"\"Extract all text and data from this document image.\n    \nReturn a JSON object with:\n- \"type\": document type (invoice, receipt, form, etc.)\n- \"text\": full extracted text\n- \"fields\": key-value pairs of important fields\n- \"tables\": any tabular data as arrays\n\nJSON:\"\"\"\n    \n    base64_image = encode_image(image_path)\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": prompt},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                        \"detail\": \"high\"\n                    }\n                }\n            ]\n        }],\n        response_format={\"type\": \"json_object\"}\n    )\n    \n    import json\n    return json.loads(response.choices[0].message.content)\n\n# Usage\ndata = extract_document_data(\"invoice.jpg\")\nprint(f\"Document type: {data['type']}\")\nprint(f\"Total: {data['fields'].get('total')}\")\n```\n\n**Compare Multiple Images:**\n```python\ndef compare_images(image_paths: list[str], prompt: str) -> str:\n    client = OpenAI()\n    \n    content = [{\"type\": \"text\", \"text\": prompt}]\n    \n    for i, path in enumerate(image_paths):\n        content.append({\n            \"type\": \"text\",\n            \"text\": f\"Image {i+1}:\"\n        })\n        content.append({\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{encode_image(path)}\",\n                \"detail\": \"low\"\n            }\n        })\n    \n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": content}],\n        max_tokens=1000\n    )\n    \n    return response.choices[0].message.content\n\n# Usage\nanalysis = compare_images(\n    [\"design_v1.png\", \"design_v2.png\"],\n    \"Compare these two UI designs. What changed? Which is better and why?\"\n)\n```",
  "prompt": "A skill for image processing with vision AI",
  "createdAt": "2024-01-15T10:00:00.000Z",
  "published": true
}
