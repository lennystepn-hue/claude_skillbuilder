{
  "id": "ai-langchain",
  "name": "langchain-helper",
  "description": "Build LLM applications with LangChain framework",
  "category": "AI",
  "content": "---\nname: langchain-helper\ndescription: Build LLM apps with LangChain\ncategory: AI\n---\n\n# LangChain Helper\n\n## Overview\nBuild powerful LLM applications using LangChain for chains, agents, RAG, and tool integration.\n\n## Activation\nActivates when user mentions \"LangChain\", \"RAG\", \"retrieval augmented\", \"LLM chain\", \"agent\", \"vector store\", or \"embeddings\".\n\n## Instructions\n\n### Core Components\n1. **LLMs/Chat Models**: Interface with language models\n2. **Prompts**: Template and manage prompts\n3. **Chains**: Combine multiple steps\n4. **Agents**: Dynamic decision-making with tools\n5. **Memory**: Maintain conversation context\n6. **Retrievers**: Fetch relevant documents\n\n### Best Practices\n- Use streaming for better UX\n- Implement proper error handling\n- Cache embeddings when possible\n- Use async for concurrent operations\n- Monitor token usage\n\n## Examples\n\n**Basic RAG Pipeline:**\n```python\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains import RetrievalQA\nfrom langchain_community.document_loaders import DirectoryLoader\n\nclass RAGPipeline:\n    def __init__(self, docs_path, persist_dir=\"./chroma_db\"):\n        self.embeddings = OpenAIEmbeddings()\n        self.llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n        self.persist_dir = persist_dir\n        self.docs_path = docs_path\n        self.vectorstore = None\n        self.qa_chain = None\n    \n    def load_and_index(self):\n        # Load documents\n        loader = DirectoryLoader(self.docs_path, glob=\"**/*.md\")\n        documents = loader.load()\n        \n        # Split into chunks\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200,\n            separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n        )\n        splits = splitter.split_documents(documents)\n        \n        # Create vector store\n        self.vectorstore = Chroma.from_documents(\n            documents=splits,\n            embedding=self.embeddings,\n            persist_directory=self.persist_dir\n        )\n        \n        # Create QA chain\n        self.qa_chain = RetrievalQA.from_chain_type(\n            llm=self.llm,\n            chain_type=\"stuff\",\n            retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n            return_source_documents=True\n        )\n        \n        return len(splits)\n    \n    def query(self, question):\n        if not self.qa_chain:\n            raise ValueError(\"Call load_and_index() first\")\n        \n        result = self.qa_chain.invoke({\"query\": question})\n        return {\n            \"answer\": result[\"result\"],\n            \"sources\": [doc.metadata for doc in result[\"source_documents\"]]\n        }\n\n# Usage\nrag = RAGPipeline(\"./docs\")\nrag.load_and_index()\nanswer = rag.query(\"How do I configure authentication?\")\n```\n\n**Agent with Tools:**\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain.tools import tool\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef search_database(query: str) -> str:\n    \"\"\"Search the database for information. Use for factual queries.\"\"\"\n    # Implement database search\n    return f\"Database results for: {query}\"\n\n@tool\ndef calculate(expression: str) -> str:\n    \"\"\"Evaluate a mathematical expression. Use for calculations.\"\"\"\n    try:\n        result = eval(expression)  # Use safer eval in production\n        return str(result)\n    except Exception as e:\n        return f\"Error: {e}\"\n\n@tool\ndef get_current_time() -> str:\n    \"\"\"Get the current date and time.\"\"\"\n    from datetime import datetime\n    return datetime.now().isoformat()\n\ndef create_agent():\n    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n    tools = [search_database, calculate, get_current_time]\n    \n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"You are a helpful assistant. Use the available tools when needed.\"),\n        (\"human\", \"{input}\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n    ])\n    \n    agent = create_openai_tools_agent(llm, tools, prompt)\n    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n    \n    return executor\n\n# Usage\nagent = create_agent()\nresult = agent.invoke({\"input\": \"What's 15% of 847?\"})\n```\n\n**Conversational Memory:**\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferWindowMemory\n\ndef create_chat_with_memory(window_size=5):\n    llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n    memory = ConversationBufferWindowMemory(k=window_size)\n    \n    chain = ConversationChain(\n        llm=llm,\n        memory=memory,\n        verbose=True\n    )\n    return chain\n\n# Usage\nchat = create_chat_with_memory()\nchat.predict(input=\"My name is Alice\")\nchat.predict(input=\"What's my name?\")  # Remembers context\n```",
  "prompt": "A skill for building LangChain applications",
  "createdAt": "2024-01-15T10:00:00.000Z",
  "published": true
}
