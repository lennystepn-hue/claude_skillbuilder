{
  "id": "ai-cost-optimization",
  "name": "llm-cost-optimization",
  "description": "Optimize LLM API costs with caching, routing, and batching",
  "category": "AI",
  "content": "---\nname: llm-cost-optimization\ndescription: Reduce LLM API costs\ncategory: AI\n---\n\n# LLM Cost Optimization\n\n## Overview\nReduce LLM API costs through caching, model routing, batching, and prompt optimization.\n\n## Activation\nActivates when user mentions \"reduce costs\", \"API costs\", \"token optimization\", \"caching\", \"cheaper model\", or \"cost effective\".\n\n## Instructions\n\n### Cost Reduction Strategies\n1. **Prompt Caching**: Cache identical/similar requests\n2. **Model Routing**: Use cheaper models when possible\n3. **Batching**: Combine multiple requests\n4. **Prompt Compression**: Reduce token count\n5. **Response Caching**: Cache deterministic responses\n\n### Model Pricing Tiers\n- **Cheap**: GPT-4o-mini, Claude Haiku, Gemini Flash\n- **Mid**: GPT-4o, Claude Sonnet\n- **Premium**: GPT-4, Claude Opus\n\n## Examples\n\n**Semantic Caching:**\n```python\nimport hashlib\nimport json\nfrom openai import OpenAI\nimport chromadb\n\nclass SemanticCache:\n    def __init__(self, similarity_threshold=0.95):\n        self.client = OpenAI()\n        self.chroma = chromadb.Client()\n        self.cache = self.chroma.create_collection(\"llm_cache\")\n        self.threshold = similarity_threshold\n        self.stats = {\"hits\": 0, \"misses\": 0, \"saved_tokens\": 0}\n    \n    def _hash(self, text: str) -> str:\n        return hashlib.md5(text.encode()).hexdigest()\n    \n    def get(self, prompt: str) -> str | None:\n        results = self.cache.query(\n            query_texts=[prompt],\n            n_results=1\n        )\n        \n        if results[\"distances\"][0] and results[\"distances\"][0][0] < (1 - self.threshold):\n            self.stats[\"hits\"] += 1\n            cached = json.loads(results[\"metadatas\"][0][0][\"response\"])\n            self.stats[\"saved_tokens\"] += cached.get(\"tokens\", 0)\n            return cached[\"content\"]\n        \n        return None\n    \n    def set(self, prompt: str, response: str, tokens: int = 0):\n        self.cache.add(\n            documents=[prompt],\n            ids=[self._hash(prompt)],\n            metadatas=[{\"response\": json.dumps({\"content\": response, \"tokens\": tokens})}]\n        )\n    \n    def query(self, prompt: str, **kwargs) -> str:\n        cached = self.get(prompt)\n        if cached:\n            return cached\n        \n        self.stats[\"misses\"] += 1\n        response = self.client.chat.completions.create(\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            **kwargs\n        )\n        \n        content = response.choices[0].message.content\n        tokens = response.usage.total_tokens\n        self.set(prompt, content, tokens)\n        \n        return content\n\n# Usage\ncache = SemanticCache()\nresult = cache.query(\"What is Python?\", model=\"gpt-4o\")\nprint(f\"Cache stats: {cache.stats}\")\n```\n\n**Smart Model Router:**\n```python\nfrom openai import OpenAI\nimport re\n\nclass ModelRouter:\n    def __init__(self):\n        self.client = OpenAI()\n        self.models = {\n            \"simple\": \"gpt-4o-mini\",  # $0.15/1M input\n            \"medium\": \"gpt-4o\",        # $2.50/1M input\n            \"complex\": \"gpt-4\"          # $30/1M input\n        }\n    \n    def classify_complexity(self, prompt: str) -> str:\n        # Simple heuristics\n        word_count = len(prompt.split())\n        has_code = bool(re.search(r'```|def |class |function', prompt))\n        has_analysis = any(w in prompt.lower() for w in ['analyze', 'compare', 'evaluate', 'critique'])\n        has_creative = any(w in prompt.lower() for w in ['write', 'create', 'design', 'imagine'])\n        \n        if word_count < 50 and not has_code and not has_analysis:\n            return \"simple\"\n        elif has_creative or (has_code and has_analysis):\n            return \"complex\"\n        else:\n            return \"medium\"\n    \n    def route(self, prompt: str, force_model: str = None) -> tuple[str, str]:\n        if force_model:\n            model = force_model\n        else:\n            complexity = self.classify_complexity(prompt)\n            model = self.models[complexity]\n        \n        response = self.client.chat.completions.create(\n            model=model,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        \n        return response.choices[0].message.content, model\n\n# Usage\nrouter = ModelRouter()\nresponse, model_used = router.route(\"What is 2+2?\")  # Uses gpt-4o-mini\nprint(f\"Used model: {model_used}\")\n```\n\n**Request Batching:**\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\nfrom typing import List\n\nclass BatchProcessor:\n    def __init__(self, batch_size=10, delay_ms=100):\n        self.client = AsyncOpenAI()\n        self.batch_size = batch_size\n        self.delay = delay_ms / 1000\n        self.queue = []\n        self.results = {}\n    \n    async def process_batch(self, prompts: List[str], model=\"gpt-4o-mini\") -> List[str]:\n        # Process in parallel with rate limiting\n        semaphore = asyncio.Semaphore(self.batch_size)\n        \n        async def process_one(prompt: str, idx: int):\n            async with semaphore:\n                await asyncio.sleep(self.delay * idx)  # Stagger requests\n                response = await self.client.chat.completions.create(\n                    model=model,\n                    messages=[{\"role\": \"user\", \"content\": prompt}]\n                )\n                return response.choices[0].message.content\n        \n        tasks = [process_one(p, i) for i, p in enumerate(prompts)]\n        return await asyncio.gather(*tasks)\n\n# Usage\nasync def main():\n    processor = BatchProcessor()\n    prompts = [f\"Summarize in one sentence: Topic {i}\" for i in range(20)]\n    results = await processor.process_batch(prompts)\n    print(f\"Processed {len(results)} requests\")\n\nasyncio.run(main())\n```\n\n**Token Counter & Budget:**\n```python\nimport tiktoken\n\nclass TokenBudget:\n    def __init__(self, daily_budget_usd=10.0):\n        self.encoder = tiktoken.encoding_for_model(\"gpt-4o\")\n        self.daily_budget = daily_budget_usd\n        self.spent_today = 0.0\n        \n        # Pricing per 1M tokens\n        self.pricing = {\n            \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00},\n            \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60}\n        }\n    \n    def count_tokens(self, text: str) -> int:\n        return len(self.encoder.encode(text))\n    \n    def estimate_cost(self, prompt: str, model: str, est_output: int = 500) -> float:\n        input_tokens = self.count_tokens(prompt)\n        pricing = self.pricing.get(model, self.pricing[\"gpt-4o\"])\n        \n        cost = (input_tokens * pricing[\"input\"] + est_output * pricing[\"output\"]) / 1_000_000\n        return cost\n    \n    def can_afford(self, estimated_cost: float) -> bool:\n        return (self.spent_today + estimated_cost) <= self.daily_budget\n    \n    def record_usage(self, input_tokens: int, output_tokens: int, model: str):\n        pricing = self.pricing.get(model, self.pricing[\"gpt-4o\"])\n        cost = (input_tokens * pricing[\"input\"] + output_tokens * pricing[\"output\"]) / 1_000_000\n        self.spent_today += cost\n        return cost\n\n# Usage\nbudget = TokenBudget(daily_budget_usd=5.0)\nest = budget.estimate_cost(\"Long prompt...\", \"gpt-4o\")\nif budget.can_afford(est):\n    # Make the API call\n    pass\n```",
  "prompt": "A skill for optimizing LLM API costs",
  "createdAt": "2024-01-15T10:00:00.000Z",
  "published": true
}
