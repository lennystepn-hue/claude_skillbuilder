{
  "id": "project-documentation",
  "name": "project-documentation",
  "description": "Create comprehensive project documentation, onboarding guides, team wikis, and technical documentation",
  "category": "Project",
  "content": "---\nname: project-documentation\ndescription: Create comprehensive project documentation, onboarding guides, team wikis, and technical documentation\ncategory: Project\n---\n\n# Project Documentation & Knowledge Management\n\n## Overview\n\nThis skill helps create comprehensive, maintainable project documentation including README files, onboarding guides, team wikis, API documentation, architecture decision records (ADRs), runbooks, and technical specifications. It emphasizes clarity, searchability, and keeping documentation in sync with code. Perfect for engineering teams, product managers, and technical writers.\n\n## Activation\n\nThis skill activates when users request:\n- Project documentation or README files\n- Onboarding guides or team wikis\n- API documentation or technical specs\n- Architecture decision records (ADRs)\n- Runbooks or operational guides\n- Contributing guidelines or development setup\n- Knowledge base articles\n- Documentation structure or organization\n\n## Instructions\n\n### Creating Documentation\n\n1. **Understand Your Audience**\n   - New engineers (onboarding)\n   - Experienced team members (reference)\n   - External contributors (open source)\n   - Stakeholders (high-level overview)\n   - End users (product documentation)\n\n2. **Choose the Right Documentation Type**\n   - **README**: Project overview, quick start\n   - **Wiki**: Comprehensive knowledge base\n   - **ADR**: Architectural decisions and rationale\n   - **Runbook**: Operational procedures\n   - **API Docs**: Endpoint specifications\n   - **Onboarding**: New team member guide\n   - **Contributing**: How to contribute code\n\n3. **Structure Documentation Effectively**\n   - Start with \"What\" and \"Why\"\n   - Include \"How\" with examples\n   - Add troubleshooting sections\n   - Keep it scannable (headers, lists, code blocks)\n   - Include diagrams where helpful\n\n4. **Maintain Documentation**\n   - Keep docs close to code (in repo)\n   - Review docs in code reviews\n   - Set up automated checks (broken links)\n   - Schedule quarterly doc reviews\n   - Encourage team contributions\n\n5. **Best Practices**\n   - Write for clarity, not cleverness\n   - Use consistent formatting\n   - Provide working code examples\n   - Include search keywords\n   - Version documentation with code\n\n### Documentation Organization\n\n```\n/docs\n  /getting-started\n    - README.md (overview)\n    - setup.md (local development)\n    - quickstart.md (5-minute guide)\n  /architecture\n    - overview.md (system design)\n    - adr/ (decision records)\n    - diagrams/ (architecture diagrams)\n  /guides\n    - contributing.md\n    - code-review.md\n    - deployment.md\n  /api\n    - authentication.md\n    - endpoints.md\n    - errors.md\n  /runbooks\n    - incident-response.md\n    - deployment-rollback.md\n    - database-recovery.md\n  /onboarding\n    - week-1.md\n    - team-norms.md\n    - resources.md\n```\n\n## Examples\n\n### Example 1: Comprehensive Project README\n\n**User Request**: \"Create a professional README for our customer analytics API project.\"\n\n**Assistant Response**:\n\n```markdown\n# Customer Analytics API\n\n[![Build Status](https://github.com/company/analytics-api/workflows/CI/badge.svg)](https://github.com/company/analytics-api/actions)\n[![Coverage](https://codecov.io/gh/company/analytics-api/branch/main/graph/badge.svg)](https://codecov.io/gh/company/analytics-api)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Version](https://img.shields.io/badge/version-2.4.0-green.svg)](CHANGELOG.md)\n\n> Enterprise-grade customer analytics and behavioral insights API powering data-driven decision making for 500+ SaaS companies.\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Features](#features)\n- [Quick Start](#quick-start)\n- [Architecture](#architecture)\n- [API Documentation](#api-documentation)\n- [Development](#development)\n- [Deployment](#deployment)\n- [Testing](#testing)\n- [Contributing](#contributing)\n- [Support](#support)\n- [License](#license)\n\n## Overview\n\nThe Customer Analytics API is a RESTful service that processes billions of customer events daily, providing real-time behavioral insights, predictive analytics, and custom reporting capabilities. Built for scale, security, and developer experience.\n\n### Key Metrics\n- **Uptime**: 99.97% (2023)\n- **Throughput**: 50K requests/second\n- **Latency**: p95 <100ms, p99 <200ms\n- **Data Processed**: 2B+ events/day\n- **Customers**: 500+ companies, 100M+ end users\n\n### Technology Stack\n\n- **Runtime**: Node.js 20 LTS\n- **Framework**: Express.js 4.18\n- **Database**: PostgreSQL 15, Redis 7\n- **Message Queue**: Apache Kafka 3.4\n- **Search**: Elasticsearch 8.9\n- **Infrastructure**: Kubernetes (AWS EKS)\n- **Monitoring**: Datadog, Sentry\n\n## Features\n\n### Core Capabilities\n\n- **Event Tracking**: Capture user actions, page views, custom events\n- **User Profiles**: 360-degree customer view with behavioral attributes\n- **Segmentation**: Dynamic user segments based on behavior and properties\n- **Funnel Analysis**: Conversion tracking and drop-off identification\n- **Cohort Analysis**: User retention and engagement over time\n- **A/B Testing**: Experiment tracking and statistical significance\n- **Predictive Analytics**: Churn prediction, LTV forecasting\n- **Real-time Dashboards**: Live metrics and custom visualizations\n\n### Enterprise Features\n\n- **SSO Integration**: SAML 2.0, OAuth 2.0\n- **RBAC**: Role-based access control with custom permissions\n- **Audit Logging**: Complete activity trail for compliance\n- **Data Export**: Scheduled exports to S3, Snowflake, BigQuery\n- **SLA**: 99.9% uptime guarantee, <100ms p95 latency\n- **Support**: 24/7 enterprise support, dedicated Slack channel\n\n## Quick Start\n\n### Prerequisites\n\n- Node.js 20+ and npm 9+\n- Docker Desktop 4.20+\n- PostgreSQL 15+ (or use Docker Compose)\n- Redis 7+ (or use Docker Compose)\n\n### Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/company/analytics-api.git\ncd analytics-api\n\n# Install dependencies\nnpm install\n\n# Set up environment variables\ncp .env.example .env\n# Edit .env with your configuration\n\n# Start dependencies (PostgreSQL, Redis, Kafka)\ndocker-compose up -d\n\n# Run database migrations\nnpm run migrate\n\n# Seed sample data (optional)\nnpm run seed\n\n# Start development server\nnpm run dev\n```\n\n### First API Request\n\n```bash\n# Track an event\ncurl -X POST http://localhost:3000/api/v1/events \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -d '{\n    \"event\": \"page_view\",\n    \"user_id\": \"user_123\",\n    \"properties\": {\n      \"page\": \"/pricing\",\n      \"referrer\": \"google\"\n    }\n  }'\n\n# Response\n{\n  \"success\": true,\n  \"event_id\": \"evt_xyz789\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\"\n}\n```\n\n### Verify Installation\n\n```bash\n# Health check\ncurl http://localhost:3000/health\n\n# Expected response\n{\n  \"status\": \"healthy\",\n  \"version\": \"2.4.0\",\n  \"services\": {\n    \"database\": \"connected\",\n    \"redis\": \"connected\",\n    \"kafka\": \"connected\"\n  }\n}\n```\n\n## Architecture\n\n### High-Level Overview\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Clients   ‚îÇ (Web, Mobile, Server)\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ    API Gateway (Kong)   ‚îÇ (Rate limiting, Auth, Routing)\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n            ‚îÇ\n       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n       ‚ñº         ‚ñº\n ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n ‚îÇ REST API ‚îÇ  ‚îÇ GraphQL  ‚îÇ\n ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ            ‚îÇ\n       ‚ñº            ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Application Layer       ‚îÇ\n‚îÇ  (Business Logic)         ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n            ‚îÇ\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚ñº       ‚ñº       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ SQL ‚îÇ ‚îÇRedis ‚îÇ ‚îÇ Kafka  ‚îÇ\n‚îÇ DB  ‚îÇ ‚îÇCache ‚îÇ ‚îÇ Queue  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò\n                      ‚îÇ\n                      ‚ñº\n              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n              ‚îÇ  Workers     ‚îÇ\n              ‚îÇ (Background) ‚îÇ\n              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                     ‚îÇ\n              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n              ‚ñº              ‚ñº\n        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n        ‚îÇElastic-  ‚îÇ  ‚îÇAnalytics ‚îÇ\n        ‚îÇsearch    ‚îÇ  ‚îÇ Engine   ‚îÇ\n        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Key Design Decisions\n\nSee [Architecture Decision Records](docs/architecture/adr/) for detailed rationale:\n\n- [ADR-001: Event-Driven Architecture](docs/architecture/adr/001-event-driven.md)\n- [ADR-002: PostgreSQL for Primary Storage](docs/architecture/adr/002-postgres.md)\n- [ADR-003: Redis Caching Strategy](docs/architecture/adr/003-redis-cache.md)\n- [ADR-004: Kafka for Event Streaming](docs/architecture/adr/004-kafka-streaming.md)\n\n### Data Flow\n\n1. **Event Ingestion**: Client sends event ‚Üí API validates ‚Üí Writes to Kafka\n2. **Processing**: Worker consumes Kafka ‚Üí Enriches data ‚Üí Stores in PostgreSQL\n3. **Indexing**: Worker indexes events in Elasticsearch for search\n4. **Aggregation**: Scheduled jobs compute metrics ‚Üí Cache in Redis\n5. **Query**: Client requests data ‚Üí Check Redis ‚Üí Query PostgreSQL/Elasticsearch\n\n## API Documentation\n\n### Authentication\n\nAll API requests require an API key in the Authorization header:\n\n```bash\nAuthorization: Bearer sk_live_abc123...\n```\n\nGet your API key from the [Dashboard](https://dashboard.analytics.company.com/settings/api).\n\n### Core Endpoints\n\n#### Track Event\n\n```http\nPOST /api/v1/events\nContent-Type: application/json\nAuthorization: Bearer YOUR_API_KEY\n\n{\n  \"event\": \"button_clicked\",\n  \"user_id\": \"user_123\",\n  \"properties\": {\n    \"button_id\": \"signup_cta\",\n    \"page\": \"/pricing\"\n  },\n  \"timestamp\": \"2024-01-15T10:30:00Z\" // optional\n}\n```\n\n#### Get User Profile\n\n```http\nGET /api/v1/users/{user_id}\nAuthorization: Bearer YOUR_API_KEY\n\nResponse:\n{\n  \"user_id\": \"user_123\",\n  \"first_seen\": \"2024-01-01T00:00:00Z\",\n  \"last_seen\": \"2024-01-15T10:30:00Z\",\n  \"properties\": {\n    \"email\": \"user@example.com\",\n    \"plan\": \"enterprise\"\n  },\n  \"segments\": [\"active_users\", \"power_users\"],\n  \"event_count\": 1247\n}\n```\n\n#### Query Events\n\n```http\nPOST /api/v1/query\nContent-Type: application/json\nAuthorization: Bearer YOUR_API_KEY\n\n{\n  \"event\": \"purchase\",\n  \"filters\": {\n    \"properties.amount\": { \"$gt\": 100 },\n    \"timestamp\": { \"$gte\": \"2024-01-01\" }\n  },\n  \"group_by\": \"properties.product_id\",\n  \"aggregate\": \"sum(properties.amount)\"\n}\n```\n\nFull API documentation: **[https://docs.analytics.company.com](https://docs.analytics.company.com)**\n\n## Development\n\n### Project Structure\n\n```\nanalytics-api/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ api/              # API routes and controllers\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ events/       # Event tracking endpoints\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ users/        # User profile endpoints\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ query/        # Analytics query endpoints\n‚îÇ   ‚îú‚îÄ‚îÄ services/         # Business logic\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ event-processor.js\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user-service.js\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analytics-engine.js\n‚îÇ   ‚îú‚îÄ‚îÄ models/           # Database models (Sequelize)\n‚îÇ   ‚îú‚îÄ‚îÄ workers/          # Background job workers\n‚îÇ   ‚îú‚îÄ‚îÄ middleware/       # Express middleware\n‚îÇ   ‚îú‚îÄ‚îÄ utils/            # Helper functions\n‚îÇ   ‚îî‚îÄ‚îÄ config/           # Configuration files\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ unit/             # Unit tests\n‚îÇ   ‚îú‚îÄ‚îÄ integration/      # Integration tests\n‚îÇ   ‚îî‚îÄ‚îÄ e2e/              # End-to-end tests\n‚îú‚îÄ‚îÄ docs/                 # Documentation\n‚îú‚îÄ‚îÄ migrations/           # Database migrations\n‚îú‚îÄ‚îÄ scripts/              # Utility scripts\n‚îî‚îÄ‚îÄ k8s/                  # Kubernetes configs\n```\n\n### Development Workflow\n\n```bash\n# Create a feature branch\ngit checkout -b feature/new-analytics-endpoint\n\n# Make changes and test locally\nnpm run dev\nnpm test\n\n# Run linting and formatting\nnpm run lint\nnpm run format\n\n# Run type checking (TypeScript)\nnpm run type-check\n\n# Create database migration if needed\nnpm run migrate:create add_new_column\n\n# Commit changes (Conventional Commits)\ngit commit -m \"feat: add cohort analysis endpoint\"\n\n# Push and create PR\ngit push origin feature/new-analytics-endpoint\n```\n\n### Code Standards\n\n- **Style**: Airbnb JavaScript Style Guide\n- **Linting**: ESLint with strict rules\n- **Formatting**: Prettier (auto-format on save)\n- **Commits**: Conventional Commits specification\n- **Testing**: Minimum 80% code coverage\n- **Type Safety**: JSDoc annotations (migrating to TypeScript)\n\n### Environment Variables\n\nSee [.env.example](.env.example) for all configuration options. Key variables:\n\n```bash\n# Application\nNODE_ENV=development\nPORT=3000\nLOG_LEVEL=debug\n\n# Database\nDATABASE_URL=postgresql://user:pass@localhost:5432/analytics\nREDIS_URL=redis://localhost:6379\n\n# Kafka\nKAFKA_BROKERS=localhost:9092\nKAFKA_CLIENT_ID=analytics-api\n\n# Authentication\nJWT_SECRET=your-secret-key\nAPI_KEY_SALT=your-salt\n\n# External Services\nDATADOG_API_KEY=your-key\nSENTRY_DSN=your-dsn\n```\n\n## Testing\n\n### Running Tests\n\n```bash\n# Run all tests\nnpm test\n\n# Run unit tests only\nnpm run test:unit\n\n# Run integration tests\nnpm run test:integration\n\n# Run with coverage\nnpm run test:coverage\n\n# Watch mode (for development)\nnpm run test:watch\n\n# Run specific test file\nnpm test -- tests/unit/event-processor.test.js\n```\n\n### Test Structure\n\n```javascript\n// tests/unit/services/event-processor.test.js\nconst { processEvent } = require('../../../src/services/event-processor');\n\ndescribe('Event Processor', () => {\n  describe('processEvent()', () => {\n    it('should validate required fields', async () => {\n      const invalidEvent = { user_id: 'user_123' }; // missing 'event'\n      \n      await expect(processEvent(invalidEvent))\n        .rejects.toThrow('Event name is required');\n    });\n\n    it('should enrich event with metadata', async () => {\n      const event = {\n        event: 'page_view',\n        user_id: 'user_123'\n      };\n\n      const processed = await processEvent(event);\n\n      expect(processed).toMatchObject({\n        event: 'page_view',\n        user_id: 'user_123',\n        timestamp: expect.any(Date),\n        event_id: expect.stringMatching(/^evt_/),\n        ip_address: expect.any(String)\n      });\n    });\n  });\n});\n```\n\n### Coverage Requirements\n\n- **Overall**: Minimum 80%\n- **Critical Paths**: 100% (authentication, billing, data integrity)\n- **New Code**: Must maintain or improve coverage\n\n## Deployment\n\n### Environments\n\n- **Development**: `dev.analytics.company.com` (auto-deploy from `develop` branch)\n- **Staging**: `staging.analytics.company.com` (auto-deploy from `main` branch)\n- **Production**: `api.analytics.company.com` (manual deploy via GitHub release)\n\n### Deployment Process\n\n```bash\n# 1. Ensure all tests pass\nnpm run test:all\n\n# 2. Create a release\ngit tag v2.5.0\ngit push origin v2.5.0\n\n# 3. GitHub Actions builds Docker image and pushes to ECR\n# 4. ArgoCD detects new image and deploys to Kubernetes\n# 5. Health checks verify deployment\n# 6. Traffic gradually shifts to new version (canary)\n\n# Rollback if needed\nkubectl rollout undo deployment/analytics-api -n production\n```\n\n### Monitoring\n\n- **Metrics**: Datadog dashboards ([Production Dashboard](https://app.datadoghq.com/dashboard/abc123))\n- **Logs**: CloudWatch Logs, searchable via Datadog\n- **Errors**: Sentry error tracking\n- **Uptime**: StatusPage.io ([status.analytics.company.com](https://status.analytics.company.com))\n- **Alerts**: PagerDuty for critical issues\n\n### Runbooks\n\nOperational guides for common scenarios:\n\n- [Deployment Rollback](docs/runbooks/deployment-rollback.md)\n- [Database Recovery](docs/runbooks/database-recovery.md)\n- [Scaling Guide](docs/runbooks/scaling.md)\n- [Incident Response](docs/runbooks/incident-response.md)\n\n## Contributing\n\nWe welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for:\n\n- Code of Conduct\n- Development setup\n- Pull request process\n- Coding standards\n- Commit message format\n\n### Quick Contribution Guide\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Add tests for your changes\n5. Ensure tests pass (`npm test`)\n6. Commit with conventional commits (`git commit -m 'feat: add amazing feature'`)\n7. Push to your fork (`git push origin feature/amazing-feature`)\n8. Open a Pull Request\n\n## Support\n\n### Documentation\n- **API Docs**: https://docs.analytics.company.com\n- **Architecture**: [docs/architecture/](docs/architecture/)\n- **FAQs**: [docs/faq.md](docs/faq.md)\n\n### Community\n- **Slack**: [#analytics-api](https://company.slack.com/archives/analytics-api) (internal)\n- **GitHub Issues**: [Bug reports and feature requests](https://github.com/company/analytics-api/issues)\n- **Stack Overflow**: Tag `company-analytics-api`\n\n### Enterprise Support\n- **Email**: support@analytics.company.com\n- **Phone**: +1 (555) 123-4567\n- **SLA**: 24/7 support with <1 hour P0 response time\n\n## Changelog\n\nSee [CHANGELOG.md](CHANGELOG.md) for version history and release notes.\n\n## License\n\nThis project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.\n\n---\n\n**Built with ‚ù§Ô∏è by the Platform Engineering team**\n\n[Report Bug](https://github.com/company/analytics-api/issues) ¬∑ [Request Feature](https://github.com/company/analytics-api/issues) ¬∑ [Documentation](https://docs.analytics.company.com)\n```\n\n---\n\n### Example 2: Engineering Onboarding Guide\n\n**User Request**: \"Create a comprehensive onboarding guide for new engineers joining our team.\"\n\n**Assistant Response**:\n\n```markdown\n# Engineering Onboarding Guide\n## Welcome to the Platform Engineering Team!\n\n> **Goal**: Get you productive and comfortable within your first two weeks. This guide will walk you through everything you need to know.\n\n## Table of Contents\n\n- [Week 1: Foundations](#week-1-foundations)\n- [Week 2: First Contributions](#week-2-first-contributions)\n- [Beyond Week 2](#beyond-week-2)\n- [Resources](#resources)\n- [FAQs](#faqs)\n\n---\n\n## Before Day 1\n\n### Administrative Setup (HR will coordinate)\n\n- [ ] Receive laptop (MacBook Pro M2)\n- [ ] Get employee badge and office access\n- [ ] Complete HR paperwork\n- [ ] Set up benefits enrollment\n\n### Access Requests (Manager will initiate)\n\n- [ ] GitHub organization access\n- [ ] AWS console access (read-only initially)\n- [ ] Slack workspace invite\n- [ ] Jira project access\n- [ ] Google Workspace account\n- [ ] VPN credentials\n- [ ] Datadog observer access\n\n---\n\n## Week 1: Foundations\n\n### Day 1: Orientation & Setup\n\n#### Morning (9:00 AM - 12:00 PM)\n\n**9:00 - 10:00**: Welcome & Team Intro\n- Meet with your manager (Sarah)\n- Team introduction (all-hands)\n- Office tour and facilities overview\n- Set up your workspace\n\n**10:00 - 12:00**: Local Development Setup\n\nFollow our [Development Environment Setup Guide](docs/guides/setup.md):\n\n```bash\n# 1. Install Homebrew\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# 2. Install Node.js and tooling\nbrew install node@20 git docker postgresql@15\n\n# 3. Configure Git\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@company.com\"\n\n# 4. Set up SSH key for GitHub\nssh-keygen -t ed25519 -C \"your.email@company.com\"\ncat ~/.ssh/id_ed25519.pub\n# Add to GitHub: https://github.com/settings/keys\n\n# 5. Clone our monorepo\ngit clone git@github.com:company/platform-monorepo.git\ncd platform-monorepo\n\n# 6. Install dependencies\nnpm install\n\n# 7. Set up environment\ncp .env.example .env.local\n# Edit .env.local with provided credentials\n\n# 8. Start services\nnpm run dev\n\n# 9. Verify everything works\ncurl http://localhost:3000/health\n# Should return: {\"status\":\"healthy\"}\n```\n\n**Checkpoint**: By lunch, you should have:\n- [ ] Laptop fully set up\n- [ ] Code running locally\n- [ ] Accessed all key tools (Slack, GitHub, Jira)\n\n#### Afternoon (1:00 PM - 5:00 PM)\n\n**1:00 - 2:00**: Lunch with the team (we'll go somewhere nearby)\n\n**2:00 - 3:30**: Architecture Overview\n- Review [Architecture Documentation](docs/architecture/overview.md)\n- Watch \"Platform Engineering Deep Dive\" (recorded)\n- Explore the codebase with your onboarding buddy (Marcus)\n\n**3:30 - 4:30**: Tools & Processes\n- **Jira**: How we track work ([guide](docs/guides/jira.md))\n- **GitHub**: Branch strategy, PR process ([guide](docs/guides/github-workflow.md))\n- **Slack**: Key channels and etiquette\n- **Datadog**: Monitoring and observability\n\n**4:30 - 5:00**: First Task\nPick up your first Jira ticket: a \"good first issue\" labeled starter task.\n- Read the ticket thoroughly\n- Ask questions in #eng-platform Slack channel\n- Start exploring the relevant code\n\n### Day 2: Deep Dive into Codebase\n\n#### Morning: Codebase Exploration\n\n**9:00 - 10:30**: Core Services Walkthrough (with Marcus)\n\nExplore each service:\n\n1. **Authentication Service** (`services/auth/`)\n   - JWT token generation and validation\n   - SSO integration (SAML, OAuth)\n   - User session management\n   - Key files: `auth-controller.js`, `jwt-service.js`\n\n2. **API Gateway** (`services/gateway/`)\n   - Request routing\n   - Rate limiting\n   - Authentication middleware\n   - Key files: `router.js`, `rate-limiter.js`\n\n3. **Analytics Service** (`services/analytics/`)\n   - Event ingestion and processing\n   - Data aggregation\n   - Query engine\n   - Key files: `event-processor.js`, `query-builder.js`\n\n**10:30 - 12:00**: Database & Data Models\n\n```bash\n# Connect to local database\npsql postgresql://localhost:5432/platform_dev\n\n# Explore schema\n\\dt  # List tables\n\\d users  # Describe users table\n\\d events  # Describe events table\n```\n\nKey data models to understand:\n- Users and authentication\n- Events and properties\n- Organizations and teams\n- API keys and permissions\n\n#### Afternoon: Development Workflow\n\n**1:00 - 2:30**: Make Your First Code Change\n\nLet's add a simple health check improvement:\n\n```bash\n# 1. Create a feature branch\ngit checkout -b <your-name>/improve-health-check\n\n# 2. Make a small change\n# Edit src/api/health.js to add database connection check\n\n# 3. Add tests\n# Edit tests/api/health.test.js\n\n# 4. Run tests locally\nnpm test\n\n# 5. Commit with conventional commit format\ngit add .\ngit commit -m \"feat: add database check to health endpoint\"\n\n# 6. Push to GitHub\ngit push origin <your-name>/improve-health-check\n\n# 7. Open a Pull Request\n# Go to GitHub and create PR\n# Request review from your onboarding buddy\n```\n\n**2:30 - 3:30**: Code Review Process\n- How we review PRs\n- What reviewers look for\n- How to give good feedback\n- Using GitHub suggestions\n\n**3:30 - 4:30**: Testing Philosophy\n- Unit tests (Jest)\n- Integration tests (Supertest)\n- E2E tests (Playwright)\n- Running tests in CI/CD\n\n**4:30 - 5:00**: Reflect and Questions\n- Daily check-in with manager\n- What went well?\n- What's confusing?\n- Blockers or concerns?\n\n### Day 3: Team Processes\n\n**9:00 - 9:30**: Daily Standup (your first one!)\n- Format: What you did yesterday, what you're doing today, any blockers\n- Keep it brief (2-3 minutes per person)\n- Post in #eng-platform Slack after\n\n**9:30 - 11:00**: Agile Workflow\n- Sprint planning process\n- Backlog grooming\n- Story pointing and estimation\n- Definition of Done\n\n**11:00 - 12:00**: Work on Starter Task\n- Continue your first Jira ticket\n- Pair with Marcus if stuck\n- Don't hesitate to ask questions!\n\n**1:00 - 2:00**: Deployment & CI/CD\n- GitHub Actions workflows\n- Deployment pipeline (dev ‚Üí staging ‚Üí production)\n- Canary deployments\n- Rollback procedures\n\n**2:00 - 3:30**: Observability & Debugging\n- Datadog dashboards\n- Log exploration\n- Error tracking in Sentry\n- Tracing distributed requests\n\n**3:30 - 5:00**: Work on Starter Task\n- Aim to complete your first PR\n- Request code review\n\n### Day 4: Domain Knowledge\n\n**9:00 - 10:30**: Product Overview\n- What we build and why\n- Customer personas\n- Business model\n- Competitive landscape\n- Roadmap and priorities\n\n**10:30 - 12:00**: Security & Compliance\n- Security best practices\n- Secrets management (AWS Secrets Manager)\n- SOC 2 compliance requirements\n- Data privacy (GDPR, CCPA)\n\n**1:00 - 2:30**: On-Call & Incident Response\n- On-call rotation (you'll shadow first)\n- PagerDuty setup\n- Runbooks and playbooks\n- Incident commander role\n\n**2:30 - 5:00**: Shadowing\n- Sit with a teammate and observe their workflow\n- Ask questions about decisions they make\n- See how they debug issues\n\n### Day 5: First Sprint\n\n**9:00 - 10:00**: Sprint Planning\n- Your first sprint planning meeting\n- Pick up 1-2 tickets for next sprint\n- Ask questions about requirements\n\n**10:00 - 12:00**: Work on Sprint Tasks\n\n**1:00 - 2:00**: Code Review Practice\n- Review someone else's PR (with guidance)\n- Learn to give constructive feedback\n\n**2:00 - 4:00**: Documentation Day\n- Update any docs you found confusing this week\n- Add your learnings to team wiki\n- Improve README if needed\n\n**4:00 - 5:00**: Week 1 Retrospective\n- Meet with manager\n- Reflect on the week\n- Adjust Week 2 plan if needed\n- Celebrate your first week! üéâ\n\n---\n\n## Week 2: First Contributions\n\n### Goals for Week 2\n\n- [ ] Merge your first non-trivial PR\n- [ ] Complete 2-3 small features or bug fixes\n- [ ] Give feedback on at least 2 PRs\n- [ ] Participate actively in team ceremonies\n- [ ] Ask lots of questions (seriously!)\n\n### Daily Routine (Days 6-10)\n\n**9:00 - 9:30**: Daily Standup\n\n**9:30 - 12:00**: Development Work\n- Work on sprint tasks\n- Pair programming sessions (at least 2x this week)\n- Code reviews\n\n**1:00 - 5:00**: Development Work\n- Continue sprint tasks\n- Testing and documentation\n- Learning and exploration\n\n### Key Activities This Week\n\n#### Mid-Week Check-in (Day 8)\n- 30-min 1:1 with manager\n- Progress on sprint tasks\n- Confidence level\n- Any concerns\n\n#### First Full PR Review (Day 9)\n- Review a substantial PR independently\n- Leave thoughtful, constructive comments\n- Ask questions if anything is unclear\n\n#### End of Week 2 (Day 10)\n- **4:00 PM**: Two-week retrospective with manager\n- **4:30 PM**: Team happy hour (casual, optional)\n\n---\n\n## Beyond Week 2\n\n### Week 3-4: Increasing Independence\n\n- Take on medium-complexity tasks\n- Start participating in architecture discussions\n- Shadow on-call (don't be primary yet)\n- Contribute to team initiatives\n\n### Month 2: Full Team Member\n\n- Work on any task in the backlog\n- Participate in sprint planning\n- Join on-call rotation (with backup support)\n- Mentor the next new hire\n\n### Month 3: Specialization\n\n- Identify your area of interest\n- Take ownership of a component or service\n- Lead a small project\n- Present at engineering all-hands\n\n---\n\n## Resources\n\n### Documentation\n\n- [Architecture Overview](docs/architecture/overview.md)\n- [API Documentation](https://docs.platform.company.com)\n- [Development Setup](docs/guides/setup.md)\n- [GitHub Workflow](docs/guides/github-workflow.md)\n- [Runbooks](docs/runbooks/)\n\n### Tools Access\n\n| Tool | URL | Purpose |\n|------|-----|----------|\n| GitHub | https://github.com/company | Code repository |\n| Jira | https://company.atlassian.net | Project management |\n| Slack | https://company.slack.com | Communication |\n| Datadog | https://app.datadoghq.com | Monitoring |\n| AWS Console | https://company.awsapps.com | Infrastructure |\n| Figma | https://figma.com/company | Design specs |\n\n### Key Slack Channels\n\n- `#eng-platform`: Team channel (daily communication)\n- `#eng-all`: All engineering announcements\n- `#eng-help`: Ask technical questions\n- `#incidents`: Production incident coordination\n- `#deploys`: Deployment notifications\n- `#wins`: Celebrate team wins\n- `#random`: Social, memes, fun\n\n### Learning Resources\n\n**Internal**:\n- Engineering wiki: https://wiki.company.com/engineering\n- Tech talks (recorded): https://company.com/tech-talks\n- Architecture Decision Records: [docs/architecture/adr/](docs/architecture/adr/)\n\n**External**:\n- Node.js Best Practices: https://github.com/goldbergyoni/nodebestpractices\n- Kubernetes Docs: https://kubernetes.io/docs/\n- Our Tech Blog: https://blog.company.com/engineering\n\n### Your Onboarding Team\n\n- **Manager**: Sarah Chen (sarah@company.com) - 1:1s every week\n- **Onboarding Buddy**: Marcus Liu (marcus@company.com) - Go-to for day-to-day questions\n- **Technical Mentor**: Alex Rodriguez (alex@company.com) - Architecture and design questions\n- **People Team**: hr@company.com - Benefits, policies, administrative\n\n---\n\n## FAQs\n\n### \"What if I don't understand something?\"\n\n**Ask immediately!** We have a \"no stupid questions\" culture. Use:\n- Your onboarding buddy (Marcus) - any question\n- `#eng-help` Slack channel - technical questions\n- Your manager (Sarah) - process or career questions\n\n### \"How much should I accomplish in my first week?\"\n\nExpectations are:\n- Week 1: Get set up, understand the system, make a tiny contribution\n- Week 2: Complete 2-3 small tasks independently\n- Month 1: Be productive on most tasks with some guidance\n\nDon't pressure yourself to ship huge features immediately!\n\n### \"When will I be on-call?\"\n\nNot for at least 4-6 weeks. You'll shadow others first, and we'll make sure you're comfortable before adding you to rotation.\n\n### \"What's the work-from-home policy?\"\n\nFlexible:\n- Default: 3 days in office, 2 remote (Tuesday, Wednesday, Thursday in-office)\n- Your first 2 weeks: Recommend coming in daily to meet the team\n- Remote work: Communicate in Slack, keep camera on in meetings\n\n### \"How do I know if I'm progressing well?\"\n\nYou'll have regular check-ins:\n- Daily standups (informal feedback)\n- Weekly 1:1s with manager\n- 30-day formal review\n- 90-day performance review\n\nIf you're concerned, ask! Feedback is a gift.\n\n### \"What if my starter task is taking too long?\"\n\nCompletely normal! Don't suffer in silence:\n- Spend max 30 minutes stuck before asking\n- Pair with your buddy\n- It's okay if it takes days - learning is the goal\n\n---\n\n## Feedback on This Guide\n\nThis guide is a living document. If you found something:\n- **Confusing**: Open an issue or PR to clarify\n- **Missing**: Add it!\n- **Helpful**: Let us know so we keep it\n\nEdit it here: `docs/onboarding/engineering-onboarding.md`\n\n---\n\n**Welcome to the team! We're excited to have you.** üöÄ\n\nYour first commit will be merged soon. Many more to come.\n```\n\n---\n\n### Example 3: Architecture Decision Record (ADR)\n\n**User Request**: \"Create an ADR for our decision to use PostgreSQL over MongoDB for our primary database.\"\n\n**Assistant Response**:\n\n```markdown\n# ADR-002: Use PostgreSQL for Primary Data Storage\n\n## Status\n\n**Accepted** - January 10, 2024\n\n## Context\n\nWe are building a customer analytics platform that will store:\n- User profiles and metadata (10M+ users)\n- Event data (billions of events)\n- Aggregated metrics and dashboards\n- Organizational data and permissions\n\nWe need to choose a primary database that will:\n1. Handle high write throughput (50K events/second)\n2. Support complex analytical queries\n3. Ensure data consistency and integrity\n4. Scale horizontally as we grow\n5. Provide strong ACID guarantees for critical data\n6. Enable rich querying without application-level joins\n\n### Options Considered\n\n1. **PostgreSQL** (Relational SQL)\n2. **MongoDB** (Document NoSQL)\n3. **MySQL** (Relational SQL)\n4. **Cassandra** (Wide-column NoSQL)\n\n## Decision\n\n**We will use PostgreSQL 15+ as our primary database.**\n\nFor specific use cases, we will supplement with:\n- **Redis**: Caching and session storage\n- **Elasticsearch**: Full-text search and log analytics\n- **S3**: Object storage for exports and archives\n\n## Rationale\n\n### Why PostgreSQL?\n\n#### 1. Relational Model Fits Our Domain\n\nOur data has clear relationships:\n- Users belong to Organizations\n- Events belong to Users\n- Dashboards have Widgets that query Events\n- Permissions are hierarchical\n\nRelational databases excel at modeling these relationships without denormalization.\n\n**Example**: Querying \"all events for users in organization X with role Y\" is simple in SQL:\n\n```sql\nSELECT e.* \nFROM events e\nJOIN users u ON e.user_id = u.id\nJOIN organization_members om ON u.id = om.user_id\nWHERE om.organization_id = 'org_123'\n  AND om.role = 'admin';\n```\n\nIn MongoDB, this requires:\n- Application-level joins (slow)\n- OR extensive denormalization (data inconsistency risk)\n- OR $lookup aggregations (complex, less performant)\n\n#### 2. ACID Guarantees for Critical Operations\n\nCritical operations require atomicity:\n- User signup (create user + organization + default settings)\n- Subscription changes (update plan + permissions + billing)\n- Data deletion (GDPR compliance - delete user + all events)\n\nPostgreSQL's transactions ensure these multi-step operations are atomic:\n\n```sql\nBEGIN;\n  INSERT INTO users (...) VALUES (...);\n  INSERT INTO organizations (...) VALUES (...);\n  INSERT INTO organization_members (...) VALUES (...);\nCOMMIT;\n```\n\nMongoDB's multi-document transactions:\n- Added in version 4.0 (relatively new, less battle-tested)\n- Performance overhead for distributed transactions\n- More complex to reason about\n\n#### 3. JSON Support for Flexible Schemas\n\nPostgreSQL's JSONB type gives us \"best of both worlds\":\n- Structured data in relational tables\n- Flexible schemas where needed (event properties)\n\n```sql\nCREATE TABLE events (\n  id SERIAL PRIMARY KEY,\n  user_id INTEGER NOT NULL,\n  event_name VARCHAR(255) NOT NULL,\n  properties JSONB,  -- Flexible, indexed, queryable\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Index JSON fields for fast queries\nCREATE INDEX idx_events_properties ON events USING GIN (properties);\n\n-- Query JSON data\nSELECT * FROM events \nWHERE properties->>'product_id' = 'prod_123'\n  AND (properties->>'amount')::numeric > 100;\n```\n\nThis matches MongoDB's flexibility while maintaining relational structure.\n\n#### 4. Analytical Query Performance\n\nAnalytics requires complex aggregations:\n- Time-series analysis (daily active users)\n- Funnel analysis (multi-step conversion rates)\n- Cohort analysis (retention by signup date)\n\nPostgreSQL excels with:\n- Window functions\n- CTEs (Common Table Expressions)\n- Efficient aggregations\n- Materialized views\n\n**Example**: 30-day retention cohort analysis\n\n```sql\nWITH cohorts AS (\n  SELECT \n    user_id,\n    DATE_TRUNC('day', MIN(created_at)) AS cohort_date\n  FROM events\n  GROUP BY user_id\n),\nuser_activities AS (\n  SELECT \n    c.cohort_date,\n    DATE_TRUNC('day', e.created_at) AS activity_date,\n    COUNT(DISTINCT e.user_id) AS active_users\n  FROM cohorts c\n  JOIN events e ON c.user_id = e.user_id\n  GROUP BY c.cohort_date, DATE_TRUNC('day', e.created_at)\n)\nSELECT \n  cohort_date,\n  activity_date,\n  active_users,\n  ROUND(100.0 * active_users / \n    FIRST_VALUE(active_users) OVER (\n      PARTITION BY cohort_date ORDER BY activity_date\n    ), 2) AS retention_percent\nFROM user_activities\nORDER BY cohort_date, activity_date;\n```\n\nThis query is concise and performant in PostgreSQL. MongoDB equivalent would require:\n- Multiple aggregation pipeline stages\n- Client-side post-processing\n- OR denormalized data (storage overhead, consistency issues)\n\n#### 5. Ecosystem and Tooling\n\n- **ORMs**: Sequelize, TypeORM, Prisma (excellent TypeScript support)\n- **Admin UIs**: pgAdmin, Postico, DBeaver\n- **Managed Services**: AWS RDS, Google Cloud SQL, Azure Database\n- **Migration Tools**: Flyway, Liquibase, Knex.js\n- **Monitoring**: pganalyze, DataDog, CloudWatch\n- **Extensions**: PostGIS (geo), pg_stat_statements (performance)\n\n#### 6. Horizontal Scalability\n\nModern PostgreSQL scales well:\n- **Read replicas**: Offload analytical queries\n- **Connection pooling**: PgBouncer for thousands of connections\n- **Partitioning**: Table partitioning for large datasets\n- **Sharding**: Citus extension for distributed PostgreSQL\n\n**Our architecture**:\n```\n     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n     ‚îÇ   Primary  ‚îÇ (Writes)\n     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n            ‚îÇ\n       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n       ‚ñº         ‚ñº\n  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n  ‚îÇReplica1‚îÇ ‚îÇReplica2‚îÇ (Reads)\n  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n#### 7. Cost Efficiency\n\n- **Storage**: Row-based storage is efficient for our access patterns\n- **Compute**: Efficient query execution plans\n- **Managed Services**: RDS pricing is competitive\n- **No vendor lock-in**: Can migrate to self-hosted if needed\n\n### Why NOT MongoDB?\n\n#### Weaknesses for Our Use Case\n\n1. **Joins are cumbersome**\n   - $lookup is slow compared to SQL joins\n   - Requires denormalization (data duplication)\n   - Application-level joins are error-prone\n\n2. **Consistency challenges**\n   - Eventual consistency by default (we need strong consistency)\n   - Multi-document transactions have overhead\n   - Schema-less can lead to data quality issues\n\n3. **Analytical queries less elegant**\n   - Aggregation pipeline is verbose\n   - No window functions (until recently)\n   - Complex analytics require client-side processing\n\n4. **Less mature tooling for analytics**\n   - BI tools (Tableau, Looker) prefer SQL\n   - Our team has more SQL expertise\n\n#### When MongoDB Would Be Better\n\n- **Document-centric data**: Truly hierarchical, self-contained documents\n- **Flexible schemas at scale**: Rapidly evolving data models\n- **Massive write throughput**: >100K writes/second (beyond our scale)\n- **Geo-distributed writes**: Multi-region write conflicts (we don't need this)\n\n### Why NOT MySQL?\n\nMySQL is a strong alternative, but PostgreSQL wins on:\n- **JSON support**: JSONB is more powerful than MySQL's JSON\n- **Advanced features**: Window functions, CTEs, better full-text search\n- **Extensibility**: Rich extension ecosystem (PostGIS, pg_trgm, etc.)\n- **Standards compliance**: PostgreSQL is more SQL-standard compliant\n\nMySQL would be acceptable but PostgreSQL is the better choice.\n\n### Why NOT Cassandra?\n\nCassandra excels at:\n- Massive write throughput (>100K/sec)\n- Multi-datacenter replication\n- Always-on availability\n\nBut has significant drawbacks:\n- **No joins**: Must denormalize everything\n- **Limited querying**: Only query by partition key efficiently\n- **Complex operations**: No transactions, no aggregations\n- **Operational complexity**: Harder to operate and debug\n\nCassandra is overkill for our scale and problematic for our query patterns.\n\n## Consequences\n\n### Positive\n\n- **Developer productivity**: SQL is well-known, ORMs are mature\n- **Data integrity**: ACID transactions prevent consistency bugs\n- **Query flexibility**: Can write complex analytics without application logic\n- **Ecosystem**: Rich tooling and community support\n- **Future-proof**: Scales to billions of rows with proper partitioning\n\n### Negative\n\n- **Vertical scaling limits**: Single-node writes are eventually bounded\n  - *Mitigation*: Partition large tables, use read replicas, Citus for sharding if needed\n\n- **Schema migrations**: Require planning and coordination\n  - *Mitigation*: Use migration tools (Knex.js), zero-downtime migration techniques\n\n- **Write throughput ceiling**: PostgreSQL maxes out around 50-100K writes/sec\n  - *Mitigation*: We're currently at 5K/sec with headroom, can batch writes, use Kafka buffer\n\n### Risks and Mitigations\n\n| Risk | Likelihood | Impact | Mitigation |\n|------|------------|--------|------------|\n| Single-point-of-failure | Medium | High | Multi-AZ deployment, automated failover |\n| Write bottleneck at scale | Low | Medium | Table partitioning, write buffering via Kafka |\n| Complex migrations | Medium | Low | Migration tool automation, staging environment testing |\n| Connection pool exhaustion | Low | Medium | PgBouncer connection pooling |\n\n## Implementation Plan\n\n### Phase 1: Foundation (Week 1-2)\n- [ ] Provision RDS PostgreSQL 15 instance (Multi-AZ)\n- [ ] Set up connection pooling (PgBouncer)\n- [ ] Configure read replicas (2 replicas)\n- [ ] Implement database migration framework (Knex.js)\n\n### Phase 2: Schema Design (Week 3-4)\n- [ ] Design core tables (users, organizations, events)\n- [ ] Create indexes for common query patterns\n- [ ] Set up table partitioning for events table\n- [ ] Document schema in README\n\n### Phase 3: Integration (Week 5-6)\n- [ ] Implement ORM layer (Sequelize)\n- [ ] Write database access layer\n- [ ] Set up automated backups\n- [ ] Configure monitoring (CloudWatch, pganalyze)\n\n### Phase 4: Testing & Optimization (Week 7-8)\n- [ ] Load testing (simulate 50K writes/sec)\n- [ ] Query performance optimization\n- [ ] Disaster recovery testing\n- [ ] Document runbooks\n\n## Alternatives Considered\n\nSee comparison matrix:\n\n| Criterion | PostgreSQL | MongoDB | MySQL | Cassandra |\n|-----------|------------|---------|-------|----------|\n| Relational model | ‚úÖ Excellent | ‚ùå Requires workarounds | ‚úÖ Good | ‚ùå No joins |\n| ACID transactions | ‚úÖ Full support | ‚ö†Ô∏è Limited | ‚úÖ Full support | ‚ùå No transactions |\n| JSON support | ‚úÖ JSONB (indexed) | ‚úÖ Native | ‚ö†Ô∏è Basic | ‚ùå None |\n| Analytical queries | ‚úÖ Window functions, CTEs | ‚ö†Ô∏è Aggregation pipeline | ‚ö†Ô∏è Limited | ‚ùå Not designed for analytics |\n| Horizontal scaling | ‚ö†Ô∏è Via Citus | ‚úÖ Native sharding | ‚ö†Ô∏è Complex | ‚úÖ Designed for scale |\n| Team expertise | ‚úÖ High | ‚ö†Ô∏è Medium | ‚úÖ High | ‚ùå Low |\n| Operational complexity | ‚úÖ Simple | ‚úÖ Simple | ‚úÖ Simple | ‚ùå Complex |\n| Ecosystem/tooling | ‚úÖ Mature | ‚úÖ Good | ‚úÖ Mature | ‚ö†Ô∏è Limited |\n| **Total Score** | **9/10** | **6/10** | **7/10** | **3/10** |\n\n## References\n\n- [PostgreSQL JSON Functions](https://www.postgresql.org/docs/15/functions-json.html)\n- [Designing Data-Intensive Applications](https://dataintensive.net/) (Chapter 2: Data Models)\n- [Use The Index, Luke!](https://use-the-index-luke.com/) (PostgreSQL performance)\n- [AWS RDS Best Practices](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_BestPractices.html)\n- Internal: [Database Benchmarks](docs/performance/database-benchmarks.md)\n\n## Approval\n\n- **Proposed by**: Alex Chen (Staff Engineer)\n- **Reviewed by**: \n  - Sarah Johnson (Engineering Manager) - Approved\n  - David Kim (Principal Engineer) - Approved\n  - Maria Garcia (CTO) - Approved\n- **Date**: January 10, 2024\n\n## Revision History\n\n| Version | Date | Author | Changes |\n|---------|------|--------|----------|\n| 1.0 | 2024-01-10 | Alex Chen | Initial version |\n| 1.1 | 2024-01-15 | Alex Chen | Added Cassandra comparison per feedback |\n\n---\n\n**Next ADR**: [ADR-003: Redis Caching Strategy](003-redis-cache.md)\n```\n\n---\n\n## Best Practices\n\n### Writing Effective Documentation\n\n1. **Start with Why**\n   - Explain the purpose before the details\n   - Help readers understand context\n   - Answer \"why should I care?\"\n\n2. **Write for Scanning**\n   - Use headings and subheadings\n   - Bullet points over paragraphs\n   - Code examples for clarity\n   - Tables for comparisons\n\n3. **Keep It Current**\n   - Review docs quarterly\n   - Update when code changes\n   - Mark outdated sections\n   - Version documentation\n\n4. **Make It Searchable**\n   - Use descriptive titles\n   - Include keywords\n   - Link related docs\n   - Add table of contents\n\n5. **Show, Don't Just Tell**\n   - Provide code examples\n   - Include screenshots\n   - Add diagrams\n   - Link to working demos\n\n### Documentation Anti-Patterns\n\n- **The Novel**: Too long, nobody reads it\n- **The Mystery**: No context, assumes too much knowledge\n- **The Fossil**: Outdated and misleading\n- **The Maze**: Poor organization, can't find anything\n- **The Jargon**: Too technical, inaccessible\n- **The Orphan**: Docs separate from code, out of sync\n\n### Tools for Great Documentation\n\n- **Markdown**: Universal, version-controlled, simple\n- **Mermaid**: Diagrams as code\n- **OpenAPI/Swagger**: API documentation\n- **JSDoc/TSDoc**: Code-level documentation\n- **Docusaurus/GitBook**: Documentation websites\n- **draw.io**: Architecture diagrams\n\n## Documentation Types\n\n### 1. README Files\n- Project overview\n- Quick start guide\n- Key features\n- Links to detailed docs\n\n### 2. Onboarding Guides\n- Step-by-step setup\n- Learning resources\n- First tasks\n- Team introductions\n\n### 3. Architecture Decision Records (ADRs)\n- Context and problem\n- Decision made\n- Rationale and trade-offs\n- Consequences\n\n### 4. Runbooks\n- Operational procedures\n- Troubleshooting steps\n- Emergency responses\n- Common tasks\n\n### 5. API Documentation\n- Endpoint specifications\n- Request/response examples\n- Authentication\n- Error codes\n\n### 6. Team Wikis\n- Team norms\n- Processes\n- Historical context\n- Reference materials\n",
  "prompt": "A skill that creates comprehensive project documentation including README files, onboarding guides, team wikis, API documentation, architecture decision records (ADRs), runbooks, and technical specifications. Emphasizes clarity, maintainability, and keeping documentation in sync with code. Perfect for engineering teams building knowledge bases and onboarding new team members.",
  "createdAt": "2024-01-15T10:00:00.000Z",
  "published": true
}
